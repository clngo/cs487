{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 8.3 Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will finish building your RNN text generator.  I found that this code actually runs pretty quickly on my MacBook without GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "seq_len = 20\n",
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colin/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to download and prepare the sonnet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘sonnets.txt’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber \"https://www.dropbox.com/scl/fi/7r68l64ijemidyb9lf80q/sonnets.txt?rlkey=udb47coatr2zbrk31hsfbr22y&dl=1\" -O sonnets.txt\n",
    "text = (open(\"sonnets.txt\").read())\n",
    "text = text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to thine own bright eyes,\n",
      " feed'st thy light's flame with self-substantial fuel,\n",
      " making a famine where abundance lies,\n",
      " thy self thy foe, to thy sweet self too cruel:\n",
      " thou that art now the world's fresh ornament,\n",
      " and only herald to the gaudy spring,\n",
      " within thine own bud buriest thy content,\n",
      " and tender churl mak'st waste in niggarding:\n",
      "   pity the world, or else this glutton be,\n",
      "   to eat the world's due, by the grave and thee.\n",
      "\n",
      " ii\n",
      "\n",
      " when forty winters shall besiege thy brow,\n",
      " and dig deep trenches in thy beauty's field,\n",
      " thy youth's proud livery so gazed on now,\n",
      " will be a tatter'd weed of small worth held:\n",
      " then being asked, where all thy beauty lies,\n",
      " where all the treasure of thy lusty days;\n",
      " to say, within thine own deep sunken eyes,\n",
      " were an all-eating shame, and thriftless praise.\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my solution for the `CharacterDataset` class.\n",
    "\n",
    "Note that it returns an entire sequence of tokens for the target (unlike what we did on Monday where it only output a single token for the target.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDataset(Dataset):\n",
    "  def __init__(self,text,seq_len=100,device='cpu'):\n",
    "    \"\"\"\n",
    "    Initialize a dataset using character tokenization.\n",
    "    Arguments:\n",
    "      text: a string containing the dataset\n",
    "      seq_len: sequence length provided by __getitem__\n",
    "      device: device for PyTorch tensors\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.seq_len = seq_len\n",
    "    self.vocabulary = ''.join(sorted(list(set(text))))\n",
    "    self.index_to_char = {n:char for n, char in enumerate(self.vocabulary)}\n",
    "    self.char_to_index = {char:n for n, char in enumerate(self.vocabulary)}\n",
    "    self.device = device\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\" Return the length of sequences in the dataset. \"\"\"\n",
    "    return len(self.text)-self.seq_len-1\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    \"\"\" Return the input and target sequences starting at given index. \"\"\"\n",
    "\n",
    "    text = self.text[idx:idx+self.seq_len+1]\n",
    "    tokens = self.encode(text)\n",
    "\n",
    "    return torch.tensor(tokens[:-1],device=self.device),torch.tensor(tokens[1:],device=self.device)\n",
    "  \n",
    "  def encode(self,text):\n",
    "    \"\"\" Encode a string to a list of integer tokens. \"\"\"\n",
    "    return list(map(self.char_to_index.get,text))\n",
    "\n",
    "  def decode(self,tokens):\n",
    "    \"\"\" Decode a list of token integers into a string. \"\"\"\n",
    "    return ''.join(list(map(self.index_to_char.get,tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CharacterDataset(text,seq_len=seq_len,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 20,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 17,\n",
       " 29,\n",
       " 26,\n",
       " 24,\n",
       " 1,\n",
       " 17,\n",
       " 12,\n",
       " 20,\n",
       " 29,\n",
       " 16,\n",
       " 30,\n",
       " 31,\n",
       " 1,\n",
       " 14,\n",
       " 29,\n",
       " 16,\n",
       " 12,\n",
       " 31,\n",
       " 32,\n",
       " 29,\n",
       " 16,\n",
       " 30,\n",
       " 1,\n",
       " 34,\n",
       " 16,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 30,\n",
       " 20,\n",
       " 29,\n",
       " 16,\n",
       " 1,\n",
       " 20,\n",
       " 25,\n",
       " 14,\n",
       " 29,\n",
       " 16,\n",
       " 12,\n",
       " 30,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 31,\n",
       " 19,\n",
       " 12,\n",
       " 31,\n",
       " 1,\n",
       " 31,\n",
       " 19,\n",
       " 16,\n",
       " 29,\n",
       " 16,\n",
       " 13,\n",
       " 36,\n",
       " 1,\n",
       " 13,\n",
       " 16,\n",
       " 12,\n",
       " 32,\n",
       " 31,\n",
       " 36,\n",
       " 3,\n",
       " 30,\n",
       " 1,\n",
       " 29,\n",
       " 26,\n",
       " 30,\n",
       " 16,\n",
       " 1,\n",
       " 24,\n",
       " 20,\n",
       " 18,\n",
       " 19,\n",
       " 31,\n",
       " 1,\n",
       " 25,\n",
       " 16,\n",
       " 33,\n",
       " 16,\n",
       " 29,\n",
       " 1,\n",
       " 15,\n",
       " 20,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 13,\n",
       " 32,\n",
       " 31,\n",
       " 1,\n",
       " 12,\n",
       " 30]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.encode(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as\n"
     ]
    }
   ],
   "source": [
    "print(ds.decode(ds.encode(text[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20]), torch.Size([20]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = ds[0]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds,shuffle=True,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my solution for the recurrent neural network (RNN) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRNN(nn.Module):\n",
    "  def __init__(self,vocabulary_size,hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocabulary_size,hidden_size)\n",
    "    self.hidden_size = hidden_size\n",
    "    self.U = nn.Linear(hidden_size,hidden_size)\n",
    "    self.W = nn.Linear(hidden_size,hidden_size)\n",
    "    self.act = nn.SiLU()\n",
    "    self.V = nn.Linear(hidden_size,vocabulary_size)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.embedding(x)\n",
    "    B,N = x.shape[:2]\n",
    "    h = torch.zeros(B,self.hidden_size).to(x.device)\n",
    "    Ux = self.U(x)\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "      Wh = self.W(h)\n",
    "      h = self.act(Ux[:,i] + Wh)\n",
    "      y.append(self.V(h))\n",
    "    return torch.stack(y,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharacterRNN(len(ds.vocabulary),hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32, 20]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dl))\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 39])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here is my code to train the model.\n",
    "\n",
    "Note that I needed to use `.view()` to reshape the model output and target, becuase the loss and metric functions want the data to have shape [B,C] not [B,N,C]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassAccuracy()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=len(ds.vocabulary))\n",
    "metric.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:17<00:00, 179.96it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 543.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.46870550513267517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:14<00:00, 210.84it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 557.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 0.4899917244911194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 239.54it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 584.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: 0.5006649494171143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:13<00:00, 229.49it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 535.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: 0.508805513381958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:14<00:00, 212.09it/s]\n",
      "100%|██████████| 3060/3060 [00:06<00:00, 467.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: 0.5122350454330444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:14<00:00, 209.98it/s]\n",
      "100%|██████████| 3060/3060 [00:06<00:00, 446.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: 0.5169149041175842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:14<00:00, 209.55it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 566.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: 0.5203076601028442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 245.97it/s]\n",
      "100%|██████████| 3060/3060 [00:06<00:00, 487.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: 0.5229532718658447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 236.07it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 570.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: 0.5253639221191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3060/3060 [00:12<00:00, 241.14it/s]\n",
      "100%|██████████| 3060/3060 [00:05<00:00, 564.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: 0.5271657705307007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  pbar = tqdm(total=len(dl))\n",
    "  for x_batch, y_batch in dl:\n",
    "    opt.zero_grad()\n",
    "\n",
    "    y_pred = model(x_batch)\n",
    "    loss = loss_fn(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    pbar.update(1)\n",
    "  pbar.close()\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  metric.reset()\n",
    "  pbar = tqdm(total=len(dl))\n",
    "  for x_batch, y_batch in dl:\n",
    "    y_pred = model(x_batch)\n",
    "    metric(y_pred.view(-1,len(ds.vocabulary)),y_batch.view(-1))\n",
    "    pbar.update(1)\n",
    "  pbar.close()\n",
    "\n",
    "  acc = metric.compute().item()\n",
    "\n",
    "  print(f'epoch {epoch}: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a deterministic function to generate text given some starter text.  The function should iteratively add characters to the prompt using the trained model.  This version should be deterministic, in that in always takes the most likely next character according to the model.\n",
    "\n",
    "Test the function by prompting it with the first 10 characters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_deterministic(model,prompt,num_to_generate=1000):\n",
    "    model.eval()\n",
    "    generated = prompt # start with the given prompt \n",
    "\n",
    "    input_seq = torch.tensor([ds.char_to_index[c] for c in prompt], device=device) # Shape: (seq_len, )\n",
    "    # Creates a word using indices; EX: \"hello\" is like [0, 1, 2, 2, 3, 4]\n",
    "\n",
    "    input_seq = input_seq.unsqueeze(0) # Shape: (1, seq_len)\n",
    "    # Adds dimension at index 0. \n",
    "\n",
    "    for _ in range(num_to_generate):\n",
    "        # nums_to_generate: number of characters/tokens to be generated\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)  # Get model predictions\n",
    "            next_token = torch.argmax(output[0, -1]).item()  # most probable character (represented as an index)\n",
    "            next_char = ds.index_to_char[next_token] \n",
    "\n",
    "            generated += next_char\n",
    "\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_token]], device=device)], dim=1) # add the new token\n",
    "            input_seq = input_seq[:, -seq_len:]  # keep only the last seq_len characters\n",
    "            # input_seq  = [0, 1, 2, 2, 3, 4] (\"hello\") -> [1, 2, 2, 3, 4, 5] (\"ello \")\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a stochastic version of the text generation function.  This one should use `torch.multinomial` to sample the next character.  Note that you will need to apply `torch.softmax` to convert the model output to probabilities.  (In my experience if you don't this you end up with a CUDA error and you end up needing to restart your kernel, so be careful!)\n",
    "\n",
    "Test the function by prompting it with the first 10 characters in the dataset, and run the generation multiple times to verify the stochastic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_stochastic(model,prompt,num_to_generate=1000, temperature=0.1):\n",
    "    model.eval()\n",
    "    generated = prompt\n",
    "    input_seq = torch.tensor([ds.char_to_index[c] for c in prompt], device=device).unsqueeze(0)\n",
    "    for _ in range(num_to_generate):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)  # Get model predictions\n",
    "\n",
    "            probabilities = torch.softmax(output[0, -1] / temperature, dim=0)  # logits to probabilities\n",
    "            # dim = 0 corresponds to vocabulary dimension C in (B X N X C)\n",
    "            # C is the number of possible classes/characters\n",
    "            # higher temperature -> Uniform probability; more random\n",
    "            # lower temperature -> More peaks; more confident\n",
    "\n",
    "            next_token = torch.multinomial(probabilities, 1).item()  # sample next character\n",
    "            next_char = ds.index_to_char[next_token]\n",
    "\n",
    "            generated += next_char\n",
    "            \n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_token]], device=device)], dim=1) # add the new token\n",
    "            input_seq = input_seq[:, -seq_len:]  # keep only the last seq_len characters\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      " from fairest creatures we desire increase,\n",
      " that thereby beauty's rose might never die,\n",
      " but as\n"
     ]
    }
   ],
   "source": [
    "print(ds.decode(ds.encode(text[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_text:  ﻿i\n",
      "\n",
      " from \n",
      "Generate Text Deterministic:  ﻿i\n",
      "\n",
      " from the summer's shall the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth the worth t\n",
      "Generate Text Stochastic:  ﻿i\n",
      "\n",
      " from a-crow:\n",
      " 'tis forgworn impwams turow:\n",
      " a gifey\n",
      " up\n",
      " repumace,\n",
      " shadisw; retrupowss coudsts,\n",
      " calth, hom if thou will,--righethel,' whicvel's plence bo.'inh touch\n",
      ", of i loveirad\n",
      " sheelg, mysish';\n",
      " ino\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_text = text[:10]\n",
    "print(\"prompt_text: \", prompt_text)\n",
    "print(\"Generate Text Deterministic: \", generate_text_deterministic(model, prompt_text, num_to_generate=200))\n",
    "print(\"Generate Text Stochastic: \", generate_text_stochastic(model, prompt_text, num_to_generate=200, temperature=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
