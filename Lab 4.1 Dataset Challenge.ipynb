{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 327, 'name': 'Phishing Websites', 'repository_url': 'https://archive.ics.uci.edu/dataset/327/phishing+websites', 'data_url': 'https://archive.ics.uci.edu/static/public/327/data.csv', 'abstract': 'This dataset collected mainly from: PhishTank archive, MillerSmiles archive, Googleâ€™s searching operators.', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 11055, 'num_features': 30, 'feature_types': ['Integer'], 'demographics': [], 'target_col': ['result'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2012, 'last_updated': 'Tue Mar 05 2024', 'dataset_doi': '10.24432/C51W2X', 'creators': ['Rami Mohammad', 'Lee McCluskey'], 'intro_paper': {'ID': 396, 'type': 'NATIVE', 'title': 'An assessment of features related to phishing websites using an automated technique', 'authors': 'R. Mohammad, F. Thabtah, L. Mccluskey', 'venue': 'International Conference for Internet Technology and Secured Transactions', 'year': 2012, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/An-assessment-of-features-related-to-phishing-using-Mohammad-Thabtah/0c0ff58063f4e078714ea74f112bc709ba9fed06', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'One of the challenges faced by our research was the unavailability of reliable training datasets. In fact this challenge faces any researcher in the field. However, although plenty of articles about predicting phishing websites have been disseminated these days, no reliable training dataset has been published publically, may be because there is no agreement in literature on the definitive features that characterize phishing webpages, hence it is difficult to shape a dataset that covers all possible features. \\r\\nIn this dataset, we shed light on the important features that have proved to be sound and effective in predicting phishing websites. In addition, we propose some new features.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'For Further information about the features see the features file in the data folder.', 'citation': None}}\n",
      "                          name     role     type demographic description  \\\n",
      "0            having_ip_address  Feature  Integer        None        None   \n",
      "1                   url_length  Feature  Integer        None        None   \n",
      "2           shortining_service  Feature  Integer        None        None   \n",
      "3             having_at_symbol  Feature  Integer        None        None   \n",
      "4     double_slash_redirecting  Feature  Integer        None        None   \n",
      "5                prefix_suffix  Feature  Integer        None        None   \n",
      "6            having_sub_domain  Feature  Integer        None        None   \n",
      "7               sslfinal_state  Feature  Integer        None        None   \n",
      "8   domain_registration_length  Feature  Integer        None        None   \n",
      "9                      favicon  Feature  Integer        None        None   \n",
      "10                        port  Feature  Integer        None        None   \n",
      "11                 https_token  Feature  Integer        None        None   \n",
      "12                 request_url  Feature  Integer        None        None   \n",
      "13               url_of_anchor  Feature  Integer        None        None   \n",
      "14               links_in_tags  Feature  Integer        None        None   \n",
      "15                         sfh  Feature  Integer        None        None   \n",
      "16         submitting_to_email  Feature  Integer        None        None   \n",
      "17                abnormal_url  Feature  Integer        None        None   \n",
      "18                    redirect  Feature  Integer        None        None   \n",
      "19                on_mouseover  Feature  Integer        None        None   \n",
      "20                  rightclick  Feature  Integer        None        None   \n",
      "21                 popupwindow  Feature  Integer        None        None   \n",
      "22                      iframe  Feature  Integer        None        None   \n",
      "23               age_of_domain  Feature  Integer        None        None   \n",
      "24                   dnsrecord  Feature  Integer        None        None   \n",
      "25                 web_traffic  Feature  Integer        None        None   \n",
      "26                   page_rank  Feature  Integer        None        None   \n",
      "27                google_index  Feature  Integer        None        None   \n",
      "28      links_pointing_to_page  Feature  Integer        None        None   \n",
      "29          statistical_report  Feature  Integer        None        None   \n",
      "30                      result   Target  Integer        None        None   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n",
      "14  None             no  \n",
      "15  None             no  \n",
      "16  None             no  \n",
      "17  None             no  \n",
      "18  None             no  \n",
      "19  None             no  \n",
      "20  None             no  \n",
      "21  None             no  \n",
      "22  None             no  \n",
      "23  None             no  \n",
      "24  None             no  \n",
      "25  None             no  \n",
      "26  None             no  \n",
      "27  None             no  \n",
      "28  None             no  \n",
      "29  None             no  \n",
      "30  None             no  \n"
     ]
    }
   ],
   "source": [
    "# https://archive.ics.uci.edu/dataset/327/phishing+websites\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "phishing_websites = fetch_ucirepo(id=327) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = phishing_websites.data.features \n",
    "y = phishing_websites.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(phishing_websites.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(phishing_websites.variables) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['having_ip_address', 'url_length', 'shortining_service',\n",
       "       'having_at_symbol', 'double_slash_redirecting', 'prefix_suffix',\n",
       "       'having_sub_domain', 'sslfinal_state', 'domain_registration_length',\n",
       "       'favicon', 'port', 'https_token', 'request_url', 'url_of_anchor',\n",
       "       'links_in_tags', 'sfh', 'submitting_to_email', 'abnormal_url',\n",
       "       'redirect', 'on_mouseover', 'rightclick', 'popupwindow', 'iframe',\n",
       "       'age_of_domain', 'dnsrecord', 'web_traffic', 'page_rank',\n",
       "       'google_index', 'links_pointing_to_page', 'statistical_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = X.isna().any(axis=1)\n",
    "X = X[~bad]\n",
    "y = y[~bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['having_ip_address', 'url_length', 'shortining_service',\n",
    "       'having_at_symbol', 'double_slash_redirecting', 'prefix_suffix',\n",
    "       'having_sub_domain', 'sslfinal_state', 'domain_registration_length',\n",
    "       'favicon', 'port', 'https_token', 'request_url', 'url_of_anchor',\n",
    "       'links_in_tags', 'sfh', 'submitting_to_email', 'abnormal_url',\n",
    "       'redirect', 'on_mouseover', 'rightclick', 'popupwindow', 'iframe',\n",
    "       'age_of_domain', 'dnsrecord', 'web_traffic', 'page_rank',\n",
    "       'google_index', 'links_pointing_to_page', 'statistical_report']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values\n",
    "X = X.values.astype('float64')\n",
    "\n",
    "X -= np.mean(X,axis=0)\n",
    "\n",
    "X = torch.tensor(X).float()\n",
    "y = np.where(y == -1, 0, y)  # Convert -1 to 0\n",
    "y = torch.tensor(y.flatten()).long()  # Convert to tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Divide data into train and test splits\n",
    "import sklearn\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=0, train_size = .75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the prepared training data ready to go, it's time to define our model.\n",
    "This will be my initial model I will use and see how well it does :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(30, 100), # 30 inputs, 1 hidden layer of size 100\n",
    "    torch.nn.ReLU(), # hidden activation function, the magic happens\n",
    "    torch.nn.Linear(100, 2) # 100 inputs, 2 outputs\n",
    ")\n",
    "\n",
    "# Create a cross-entropy loss function and a stochastic gradient descent (SGD) optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "opt = torch.optim.SGD(mlp_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6811\n",
      "Epoch 2/100, Loss: 0.6761\n",
      "Epoch 3/100, Loss: 0.6713\n",
      "Epoch 4/100, Loss: 0.6667\n",
      "Epoch 5/100, Loss: 0.6621\n",
      "Epoch 6/100, Loss: 0.6576\n",
      "Epoch 7/100, Loss: 0.6533\n",
      "Epoch 8/100, Loss: 0.6490\n",
      "Epoch 9/100, Loss: 0.6439\n",
      "Epoch 10/100, Loss: 0.6399\n",
      "Epoch 11/100, Loss: 0.6353\n",
      "Epoch 12/100, Loss: 0.6311\n",
      "Epoch 13/100, Loss: 0.6281\n",
      "Epoch 14/100, Loss: 0.6234\n",
      "Epoch 15/100, Loss: 0.6194\n",
      "Epoch 16/100, Loss: 0.6155\n",
      "Epoch 17/100, Loss: 0.6116\n",
      "Epoch 18/100, Loss: 0.6077\n",
      "Epoch 19/100, Loss: 0.6041\n",
      "Epoch 20/100, Loss: 0.6005\n",
      "Epoch 21/100, Loss: 0.5966\n",
      "Epoch 22/100, Loss: 0.5927\n",
      "Epoch 23/100, Loss: 0.5890\n",
      "Epoch 24/100, Loss: 0.5858\n",
      "Epoch 25/100, Loss: 0.5820\n",
      "Epoch 26/100, Loss: 0.5787\n",
      "Epoch 27/100, Loss: 0.5749\n",
      "Epoch 28/100, Loss: 0.5712\n",
      "Epoch 29/100, Loss: 0.5678\n",
      "Epoch 30/100, Loss: 0.5645\n",
      "Epoch 31/100, Loss: 0.5609\n",
      "Epoch 32/100, Loss: 0.5581\n",
      "Epoch 33/100, Loss: 0.5545\n",
      "Epoch 34/100, Loss: 0.5513\n",
      "Epoch 35/100, Loss: 0.5476\n",
      "Epoch 36/100, Loss: 0.5445\n",
      "Epoch 37/100, Loss: 0.5411\n",
      "Epoch 38/100, Loss: 0.5380\n",
      "Epoch 39/100, Loss: 0.5353\n",
      "Epoch 40/100, Loss: 0.5319\n",
      "Epoch 41/100, Loss: 0.5284\n",
      "Epoch 42/100, Loss: 0.5256\n",
      "Epoch 43/100, Loss: 0.5225\n",
      "Epoch 44/100, Loss: 0.5186\n",
      "Epoch 45/100, Loss: 0.5158\n",
      "Epoch 46/100, Loss: 0.5132\n",
      "Epoch 47/100, Loss: 0.5106\n",
      "Epoch 48/100, Loss: 0.5067\n",
      "Epoch 49/100, Loss: 0.5035\n",
      "Epoch 50/100, Loss: 0.5004\n",
      "Epoch 51/100, Loss: 0.4980\n",
      "Epoch 52/100, Loss: 0.4943\n",
      "Epoch 53/100, Loss: 0.4921\n",
      "Epoch 54/100, Loss: 0.4888\n",
      "Epoch 55/100, Loss: 0.4856\n",
      "Epoch 56/100, Loss: 0.4832\n",
      "Epoch 57/100, Loss: 0.4805\n",
      "Epoch 58/100, Loss: 0.4778\n",
      "Epoch 59/100, Loss: 0.4751\n",
      "Epoch 60/100, Loss: 0.4736\n",
      "Epoch 61/100, Loss: 0.4696\n",
      "Epoch 62/100, Loss: 0.4661\n",
      "Epoch 63/100, Loss: 0.4638\n",
      "Epoch 64/100, Loss: 0.4614\n",
      "Epoch 65/100, Loss: 0.4584\n",
      "Epoch 66/100, Loss: 0.4554\n",
      "Epoch 67/100, Loss: 0.4528\n",
      "Epoch 68/100, Loss: 0.4506\n",
      "Epoch 69/100, Loss: 0.4471\n",
      "Epoch 70/100, Loss: 0.4452\n",
      "Epoch 71/100, Loss: 0.4430\n",
      "Epoch 72/100, Loss: 0.4410\n",
      "Epoch 73/100, Loss: 0.4371\n",
      "Epoch 74/100, Loss: 0.4354\n",
      "Epoch 75/100, Loss: 0.4327\n",
      "Epoch 76/100, Loss: 0.4295\n",
      "Epoch 77/100, Loss: 0.4272\n",
      "Epoch 78/100, Loss: 0.4261\n",
      "Epoch 79/100, Loss: 0.4229\n",
      "Epoch 80/100, Loss: 0.4205\n",
      "Epoch 81/100, Loss: 0.4185\n",
      "Epoch 82/100, Loss: 0.4166\n",
      "Epoch 83/100, Loss: 0.4146\n",
      "Epoch 84/100, Loss: 0.4118\n",
      "Epoch 85/100, Loss: 0.4093\n",
      "Epoch 86/100, Loss: 0.4065\n",
      "Epoch 87/100, Loss: 0.4046\n",
      "Epoch 88/100, Loss: 0.4021\n",
      "Epoch 89/100, Loss: 0.4003\n",
      "Epoch 90/100, Loss: 0.3974\n",
      "Epoch 91/100, Loss: 0.3952\n",
      "Epoch 92/100, Loss: 0.3938\n",
      "Epoch 93/100, Loss: 0.3910\n",
      "Epoch 94/100, Loss: 0.3894\n",
      "Epoch 95/100, Loss: 0.3873\n",
      "Epoch 96/100, Loss: 0.3850\n",
      "Epoch 97/100, Loss: 0.3827\n",
      "Epoch 98/100, Loss: 0.3807\n",
      "Epoch 99/100, Loss: 0.3791\n",
      "Epoch 100/100, Loss: 0.3778\n"
     ]
    }
   ],
   "source": [
    "batch = 32\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch, shuffle=False)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_X, batch_y in train_dataloader:\n",
    "        # print(\"batch_y: \", batch_y.shape)\n",
    "        # print(\"torch: \", torch.unique(y))\n",
    "        opt.zero_grad()  # Zero out gradients\n",
    "\n",
    "        z = mlp_model(batch_X)  # Forward pass\n",
    "        loss = loss_fn(z, batch_y)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        opt.step()  # Apply gradients\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Train Accuracy: 0.91038475455313\n",
      "MLP Test Accuracy: 0.8990593342981187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            z = model(batch_X)\n",
    "            sample, predicted_labels = torch.max(z, dim=1)\n",
    "            correct += (predicted_labels == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    return correct/total\n",
    "print(f\"MLP Train Accuracy: {accuracy(mlp_model, train_dataloader)}\")\n",
    "print(f\"MLP Test Accuracy: {accuracy(mlp_model, test_dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my accuracy is.\n",
    "I now have made some modifications in attempt to improve my model with higher training and test accuracy.\n",
    "The main changes are:\n",
    "- Raising learning rate from 1e-4 to 1e-3\n",
    "- Using Adapative Moment Estimation (ADAM) optimizer\n",
    "- Including a weight decay of 1e-4 to my optimizer\n",
    "- Having 1000 epochs. However, after testing with even 100 or 200 epochs, the change in accuracy was insignificant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(30, 100), # 30 inputs, 1 hidden layer of size 100\n",
    "    torch.nn.ReLU(), # hidden activation function, the magic happens\n",
    "    torch.nn.Linear(100, 2) # 100 inputs, 2 outputs\n",
    ")\n",
    "\n",
    "# Create a cross-entropy loss function and a stochastic gradient descent (SGD) optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "opt = torch.optim.Adam(mlp_model.parameters(), lr=lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.2772\n",
      "Epoch 2/100, Loss: 0.1630\n",
      "Epoch 3/100, Loss: 0.1483\n",
      "Epoch 4/100, Loss: 0.1376\n",
      "Epoch 5/100, Loss: 0.1275\n",
      "Epoch 6/100, Loss: 0.1190\n",
      "Epoch 7/100, Loss: 0.1121\n",
      "Epoch 8/100, Loss: 0.1095\n",
      "Epoch 9/100, Loss: 0.1028\n",
      "Epoch 10/100, Loss: 0.0967\n",
      "Epoch 11/100, Loss: 0.0930\n",
      "Epoch 12/100, Loss: 0.0886\n",
      "Epoch 13/100, Loss: 0.0872\n",
      "Epoch 14/100, Loss: 0.0864\n",
      "Epoch 15/100, Loss: 0.0806\n",
      "Epoch 16/100, Loss: 0.0778\n",
      "Epoch 17/100, Loss: 0.0749\n",
      "Epoch 18/100, Loss: 0.0732\n",
      "Epoch 19/100, Loss: 0.0718\n",
      "Epoch 20/100, Loss: 0.0706\n",
      "Epoch 21/100, Loss: 0.0685\n",
      "Epoch 22/100, Loss: 0.0660\n",
      "Epoch 23/100, Loss: 0.0651\n",
      "Epoch 24/100, Loss: 0.0635\n",
      "Epoch 25/100, Loss: 0.0621\n",
      "Epoch 26/100, Loss: 0.0604\n",
      "Epoch 27/100, Loss: 0.0592\n",
      "Epoch 28/100, Loss: 0.0576\n",
      "Epoch 29/100, Loss: 0.0566\n",
      "Epoch 30/100, Loss: 0.0548\n",
      "Epoch 31/100, Loss: 0.0556\n",
      "Epoch 32/100, Loss: 0.0550\n",
      "Epoch 33/100, Loss: 0.0521\n",
      "Epoch 34/100, Loss: 0.0527\n",
      "Epoch 35/100, Loss: 0.0512\n",
      "Epoch 36/100, Loss: 0.0511\n",
      "Epoch 37/100, Loss: 0.0517\n",
      "Epoch 38/100, Loss: 0.0522\n",
      "Epoch 39/100, Loss: 0.0486\n",
      "Epoch 40/100, Loss: 0.0477\n",
      "Epoch 41/100, Loss: 0.0479\n",
      "Epoch 42/100, Loss: 0.0484\n",
      "Epoch 43/100, Loss: 0.0459\n",
      "Epoch 44/100, Loss: 0.0466\n",
      "Epoch 45/100, Loss: 0.0455\n",
      "Epoch 46/100, Loss: 0.0453\n",
      "Epoch 47/100, Loss: 0.0465\n",
      "Epoch 48/100, Loss: 0.0460\n",
      "Epoch 49/100, Loss: 0.0444\n",
      "Epoch 50/100, Loss: 0.0443\n",
      "Epoch 51/100, Loss: 0.0430\n",
      "Epoch 52/100, Loss: 0.0436\n",
      "Epoch 53/100, Loss: 0.0417\n",
      "Epoch 54/100, Loss: 0.0430\n",
      "Epoch 55/100, Loss: 0.0441\n",
      "Epoch 56/100, Loss: 0.0445\n",
      "Epoch 57/100, Loss: 0.0442\n",
      "Epoch 58/100, Loss: 0.0401\n",
      "Epoch 59/100, Loss: 0.0398\n",
      "Epoch 60/100, Loss: 0.0402\n",
      "Epoch 61/100, Loss: 0.0398\n",
      "Epoch 62/100, Loss: 0.0405\n",
      "Epoch 63/100, Loss: 0.0400\n",
      "Epoch 64/100, Loss: 0.0398\n",
      "Epoch 65/100, Loss: 0.0402\n",
      "Epoch 66/100, Loss: 0.0398\n",
      "Epoch 67/100, Loss: 0.0390\n",
      "Epoch 68/100, Loss: 0.0389\n",
      "Epoch 69/100, Loss: 0.0384\n",
      "Epoch 70/100, Loss: 0.0386\n",
      "Epoch 71/100, Loss: 0.0398\n",
      "Epoch 72/100, Loss: 0.0397\n",
      "Epoch 73/100, Loss: 0.0383\n",
      "Epoch 74/100, Loss: 0.0373\n",
      "Epoch 75/100, Loss: 0.0370\n",
      "Epoch 76/100, Loss: 0.0390\n",
      "Epoch 77/100, Loss: 0.0389\n",
      "Epoch 78/100, Loss: 0.0385\n",
      "Epoch 79/100, Loss: 0.0355\n",
      "Epoch 80/100, Loss: 0.0363\n",
      "Epoch 81/100, Loss: 0.0354\n",
      "Epoch 82/100, Loss: 0.0366\n",
      "Epoch 83/100, Loss: 0.0360\n",
      "Epoch 84/100, Loss: 0.0368\n",
      "Epoch 85/100, Loss: 0.0371\n",
      "Epoch 86/100, Loss: 0.0365\n",
      "Epoch 87/100, Loss: 0.0366\n",
      "Epoch 88/100, Loss: 0.0351\n",
      "Epoch 89/100, Loss: 0.0365\n",
      "Epoch 90/100, Loss: 0.0362\n",
      "Epoch 91/100, Loss: 0.0351\n",
      "Epoch 92/100, Loss: 0.0346\n",
      "Epoch 93/100, Loss: 0.0337\n",
      "Epoch 94/100, Loss: 0.0364\n",
      "Epoch 95/100, Loss: 0.0348\n",
      "Epoch 96/100, Loss: 0.0367\n",
      "Epoch 97/100, Loss: 0.0335\n",
      "Epoch 98/100, Loss: 0.0335\n",
      "Epoch 99/100, Loss: 0.0352\n",
      "Epoch 100/100, Loss: 0.0351\n"
     ]
    }
   ],
   "source": [
    "batch = 32\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch, shuffle=False)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_X, batch_y in train_dataloader:\n",
    "        # print(\"batch_y: \", batch_y.shape)\n",
    "        # print(\"torch: \", torch.unique(y))\n",
    "        opt.zero_grad()  # Zero out gradients\n",
    "\n",
    "        z = mlp_model(batch_X)  # Forward pass\n",
    "        loss = loss_fn(z, batch_y)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        opt.step()  # Apply gradients\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Train Accuracy: 0.988421179592329\n",
      "MLP Test Accuracy: 0.9663531114327062\n"
     ]
    }
   ],
   "source": [
    "print(f\"MLP Train Accuracy: {accuracy(mlp_model, train_dataloader)}\")\n",
    "print(f\"MLP Test Accuracy: {accuracy(mlp_model, test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learned that the Adam optimizer greatly improved my training and test accuracies to 99%. I did try adjusting my model to have more layers. Including 2 or 3 layers lowered my training and test accuracy from 90% to 80%. Including 7 layers and more will lower the training and test accuracy to 55%. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNabx6sCRInWIIstR96Z5ey",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
