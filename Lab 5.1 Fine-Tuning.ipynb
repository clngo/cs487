{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6uOfRzgAOal"
      },
      "source": [
        "# Lab 5.1 Fine-Tuning\n",
        "\n",
        "In this notebook you will explore fine-tuning a CNN to classify pet breeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "#device = 'cuda' # set this if using GPU on Colab for example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzt-DP1N2Eb"
      },
      "source": [
        "Here is some code to download a prepared version of the [Oxford-IIIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7DK-ZnOphy1",
        "outputId": "fd0fd4e5-da41-4d53-f53c-663facfb3b14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('oxford_pets.zip'):\n",
        "  !wget \"https://www.dropbox.com/scl/fi/p49ifha27c2u3uptfj42w/oxford_pets_corrected.zip?rlkey=dwk3dsptzir8v846imsq6bgw3&dl=1\" -O oxford_pets.zip\n",
        "  !unzip -qq oxford_pets.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rysmvQcVme6i",
        "outputId": "5dfb9308-5fdc-4625-ace3-e723b41f7c1a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchmetrics\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code sets up dataset and dataloaders for the data.  It creates two versions: the `no_transform` version does not apply any transformations, and the other one applies the appropriate transformations for the VGG16 model (resize, recropping, centering, and scaling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2OU-ZaAN2Ed"
      },
      "outputs": [],
      "source": [
        "transforms = torchvision.models.VGG16_Weights.IMAGENET1K_V1.transforms()\n",
        "\n",
        "train_ds_no_transform = torchvision.datasets.ImageFolder(\n",
        "    'oxford_pets/train',\n",
        ")\n",
        "train_ds = torchvision.datasets.ImageFolder(\n",
        "    'oxford_pets/train',\n",
        "    transform=transforms\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_ds_no_transform = torchvision.datasets.ImageFolder(\n",
        "    'oxford_pets/test',\n",
        ")\n",
        "test_ds = torchvision.datasets.ImageFolder(\n",
        "    'oxford_pets/test',\n",
        "    transform=transforms\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzc2ZyNbEM9f",
        "outputId": "d0191fd9-6b43-4796-bf99-a44cca8ca27e"
      },
      "outputs": [],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "-NhwQ_5eFm_Y",
        "outputId": "561f7f69-6fc9-49e5-ba6d-a76114c63eae"
      },
      "outputs": [],
      "source": [
        "image,label = next(iter(train_ds_no_transform))\n",
        "plt.imshow(image)\n",
        "plt.title(train_ds.classes[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will grab the VGG16 model with pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TKMi61_ml8v",
        "outputId": "3a4cb48a-1561-43ec-eeab-6937cc4925ab"
      },
      "outputs": [],
      "source": [
        "vgg16 = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FKxOHhN2Ec"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Btj59JlN2Ed"
      },
      "source": [
        "1. Iterate through the parameters of the `features` part of VGG16 and set `requires_grad=False` on each paramter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXVqqX8yN2Ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Replace the `classifier` part of the network with a new `Sequential` model that has 35 outputs for the 35 pet breeds.\n",
        "\n",
        "*Note:* You can do `vgg16.classifier = nn.Sequential(...).to(device)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv2bIqGyEM9g",
        "outputId": "7ad9b824-0025-4fec-afd3-599905f8e25e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI_7Dm4oN2Ee"
      },
      "source": [
        "3. Use the following code to check the test set accuracy of the model *before* training it.  Interpret the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTnAvRKk_MG_",
        "outputId": "98e30258-aa7b-4f3b-8a2a-338ddcaa022f"
      },
      "outputs": [],
      "source": [
        "metric = torchmetrics.Accuracy('multiclass',num_classes=35).to(device)\n",
        "metric.reset()\n",
        "for x_batch, y_batch in test_loader:\n",
        "    y_pred = vgg16(x_batch.to(device))\n",
        "    metric(y_pred, y_batch.to(device))\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNb41ImN2Ee"
      },
      "source": [
        "5. Now train the model on the training set for three epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZJq3eqiAN_d",
        "outputId": "4e28446d-f96d-4f46-d05a-4e48fd437448"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6bQXPSKN2Ee"
      },
      "source": [
        "6. Evaluate the accuracy of the fine-tuned model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVPh1cjk0i2u",
        "outputId": "20efdb6e-4834-45fe-b639-531bde930f19"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHFu1h7N2Ee"
      },
      "source": [
        "7. The following code will show some test images with the correct and predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg_WzjG82DBK"
      },
      "outputs": [],
      "source": [
        "vgg16.eval()\n",
        "preds = []\n",
        "for x_batch, y_batch in test_loader:\n",
        "    y_pred = vgg16(x_batch.to(device))\n",
        "    y_pred = torch.argmax(y_pred,dim=-1)\n",
        "    preds.append(y_pred.detach().cpu().numpy())\n",
        "preds = np.concatenate(preds,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ocfrdyDSrZQD",
        "outputId": "e4c37a2d-76ab-4b49-d4b4-7faabb47c634"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for (im,label),pred in zip(test_ds_no_transform,preds):\n",
        "  if i%20 == 0:\n",
        "    plt.imshow(im)\n",
        "    plt.title(f'correct: {test_ds.classes[label]} | predicted: {test_ds.classes[pred]}')\n",
        "    plt.show()\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXpREW9Bx7Pe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
