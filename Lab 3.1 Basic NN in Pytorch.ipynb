{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5.,requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6.,requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 1.1159321069717407\n",
      "epoch 1: loss is 0.6858724355697632\n",
      "epoch 2: loss is 0.4304100573062897\n",
      "epoch 3: loss is 0.31856605410575867\n",
      "epoch 4: loss is 0.27364620566368103\n",
      "epoch 5: loss is 0.25274404883384705\n",
      "epoch 6: loss is 0.2410580962896347\n",
      "epoch 7: loss is 0.2334849089384079\n",
      "epoch 8: loss is 0.22800186276435852\n",
      "epoch 9: loss is 0.2236955165863037\n",
      "epoch 10: loss is 0.2201092392206192\n",
      "epoch 11: loss is 0.21699605882167816\n",
      "epoch 12: loss is 0.21421390771865845\n",
      "epoch 13: loss is 0.2116764783859253\n",
      "epoch 14: loss is 0.20932897925376892\n",
      "epoch 15: loss is 0.20713503658771515\n",
      "epoch 16: loss is 0.20506957173347473\n",
      "epoch 17: loss is 0.2031146138906479\n",
      "epoch 18: loss is 0.20125681161880493\n",
      "epoch 19: loss is 0.1994858682155609\n",
      "epoch 20: loss is 0.19779366254806519\n",
      "epoch 21: loss is 0.19617345929145813\n",
      "epoch 22: loss is 0.19461971521377563\n",
      "epoch 23: loss is 0.19312767684459686\n",
      "epoch 24: loss is 0.19169317185878754\n",
      "epoch 25: loss is 0.1903124898672104\n",
      "epoch 26: loss is 0.18898245692253113\n",
      "epoch 27: loss is 0.18770000338554382\n",
      "epoch 28: loss is 0.18646250665187836\n",
      "epoch 29: loss is 0.18526747822761536\n",
      "epoch 30: loss is 0.18411266803741455\n",
      "epoch 31: loss is 0.1829959750175476\n",
      "epoch 32: loss is 0.1819155067205429\n",
      "epoch 33: loss is 0.1808694303035736\n",
      "epoch 34: loss is 0.1798560619354248\n",
      "epoch 35: loss is 0.17887385189533234\n",
      "epoch 36: loss is 0.1779213547706604\n",
      "epoch 37: loss is 0.17699721455574036\n",
      "epoch 38: loss is 0.17610011994838715\n",
      "epoch 39: loss is 0.17522889375686646\n",
      "epoch 40: loss is 0.17438237369060516\n",
      "epoch 41: loss is 0.1735595315694809\n",
      "epoch 42: loss is 0.17275932431221008\n",
      "epoch 43: loss is 0.1719808578491211\n",
      "epoch 44: loss is 0.1712232083082199\n",
      "epoch 45: loss is 0.17048552632331848\n",
      "epoch 46: loss is 0.16976699233055115\n",
      "epoch 47: loss is 0.169066920876503\n",
      "epoch 48: loss is 0.16838456690311432\n",
      "epoch 49: loss is 0.16771920025348663\n",
      "epoch 50: loss is 0.16707025468349457\n",
      "epoch 51: loss is 0.1664370745420456\n",
      "epoch 52: loss is 0.16581909358501434\n",
      "epoch 53: loss is 0.16521574556827545\n",
      "epoch 54: loss is 0.16462650895118713\n",
      "epoch 55: loss is 0.16405092179775238\n",
      "epoch 56: loss is 0.1634884476661682\n",
      "epoch 57: loss is 0.1629386842250824\n",
      "epoch 58: loss is 0.16240118443965912\n",
      "epoch 59: loss is 0.16187548637390137\n",
      "epoch 60: loss is 0.16136127710342407\n",
      "epoch 61: loss is 0.1608581244945526\n",
      "epoch 62: loss is 0.16036568582057953\n",
      "epoch 63: loss is 0.15988361835479736\n",
      "epoch 64: loss is 0.15941159427165985\n",
      "epoch 65: loss is 0.15894928574562073\n",
      "epoch 66: loss is 0.15849637985229492\n",
      "epoch 67: loss is 0.15805263817310333\n",
      "epoch 68: loss is 0.1576177477836609\n",
      "epoch 69: loss is 0.1571914255619049\n",
      "epoch 70: loss is 0.1567734330892563\n",
      "epoch 71: loss is 0.1563635766506195\n",
      "epoch 72: loss is 0.15596149861812592\n",
      "epoch 73: loss is 0.15556709468364716\n",
      "epoch 74: loss is 0.15518008172512054\n",
      "epoch 75: loss is 0.15480026602745056\n",
      "epoch 76: loss is 0.1544274240732193\n",
      "epoch 77: loss is 0.15406139194965363\n",
      "epoch 78: loss is 0.1537020057439804\n",
      "epoch 79: loss is 0.15334902703762054\n",
      "epoch 80: loss is 0.1530022919178009\n",
      "epoch 81: loss is 0.15266168117523193\n",
      "epoch 82: loss is 0.15232700109481812\n",
      "epoch 83: loss is 0.15199808776378632\n",
      "epoch 84: loss is 0.15167482197284698\n",
      "epoch 85: loss is 0.1513570100069046\n",
      "epoch 86: loss is 0.15104453265666962\n",
      "epoch 87: loss is 0.15073728561401367\n",
      "epoch 88: loss is 0.15043511986732483\n",
      "epoch 89: loss is 0.15013788640499115\n",
      "epoch 90: loss is 0.14984551072120667\n",
      "epoch 91: loss is 0.14955785870552063\n",
      "epoch 92: loss is 0.14927473664283752\n",
      "epoch 93: loss is 0.14899615943431854\n",
      "epoch 94: loss is 0.14872194826602936\n",
      "epoch 95: loss is 0.1484519988298416\n",
      "epoch 96: loss is 0.14818625152111053\n",
      "epoch 97: loss is 0.14792457222938538\n",
      "epoch 98: loss is 0.14766688644886017\n",
      "epoch 99: loss is 0.14741308987140656\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new changes\n",
    "mlp_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 100), # 2 inputs, 1 hidden layer of size 100\n",
    "    \n",
    "    # hidden activation function, the magic happens\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    torch.nn.Linear(100, 3) # 100 inputs, 3 outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yippie, we have the model\n",
    "\n",
    "# time to optimize the loss function\n",
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(mlp_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 0.12434127181768417\n",
      "epoch 1: loss is 0.12423999607563019\n",
      "epoch 2: loss is 0.12414035201072693\n",
      "epoch 3: loss is 0.1240423172712326\n",
      "epoch 4: loss is 0.12394576519727707\n",
      "epoch 5: loss is 0.12385069578886032\n",
      "epoch 6: loss is 0.12375707179307938\n",
      "epoch 7: loss is 0.12366484105587006\n",
      "epoch 8: loss is 0.12357396632432938\n",
      "epoch 9: loss is 0.12348445504903793\n",
      "epoch 10: loss is 0.12339618057012558\n",
      "epoch 11: loss is 0.1233091950416565\n",
      "epoch 12: loss is 0.12322341650724411\n",
      "epoch 13: loss is 0.12313883751630783\n",
      "epoch 14: loss is 0.12305545806884766\n",
      "epoch 15: loss is 0.12297313660383224\n",
      "epoch 16: loss is 0.12289175391197205\n",
      "epoch 17: loss is 0.12281142920255661\n",
      "epoch 18: loss is 0.12273216992616653\n",
      "epoch 19: loss is 0.12265397608280182\n",
      "epoch 20: loss is 0.12257673591375351\n",
      "epoch 21: loss is 0.12250050157308578\n",
      "epoch 22: loss is 0.12242519110441208\n",
      "epoch 23: loss is 0.12235084921121597\n",
      "epoch 24: loss is 0.1222773939371109\n",
      "epoch 25: loss is 0.12220485508441925\n",
      "epoch 26: loss is 0.12213316559791565\n",
      "epoch 27: loss is 0.12206234037876129\n",
      "epoch 28: loss is 0.12199235707521439\n",
      "epoch 29: loss is 0.12192318588495255\n",
      "epoch 30: loss is 0.12185483425855637\n",
      "epoch 31: loss is 0.12178724259138107\n",
      "epoch 32: loss is 0.12172042578458786\n",
      "epoch 33: loss is 0.12165439873933792\n",
      "epoch 34: loss is 0.1215890422463417\n",
      "epoch 35: loss is 0.12152444571256638\n",
      "epoch 36: loss is 0.12146054208278656\n",
      "epoch 37: loss is 0.12139730900526047\n",
      "epoch 38: loss is 0.12133477628231049\n",
      "epoch 39: loss is 0.12127287685871124\n",
      "epoch 40: loss is 0.12121164053678513\n",
      "epoch 41: loss is 0.12115100771188736\n",
      "epoch 42: loss is 0.1210910826921463\n",
      "epoch 43: loss is 0.12103166431188583\n",
      "epoch 44: loss is 0.12097293883562088\n",
      "epoch 45: loss is 0.12091471999883652\n",
      "epoch 46: loss is 0.1208571270108223\n",
      "epoch 47: loss is 0.12080007791519165\n",
      "epoch 48: loss is 0.12074358761310577\n",
      "epoch 49: loss is 0.12068764865398407\n",
      "epoch 50: loss is 0.12063223868608475\n",
      "epoch 51: loss is 0.12057733535766602\n",
      "epoch 52: loss is 0.12052295356988907\n",
      "epoch 53: loss is 0.12046914547681808\n",
      "epoch 54: loss is 0.12041575461626053\n",
      "epoch 55: loss is 0.12036292254924774\n",
      "epoch 56: loss is 0.12031050026416779\n",
      "epoch 57: loss is 0.12025860697031021\n",
      "epoch 58: loss is 0.12020712345838547\n",
      "epoch 59: loss is 0.12015616148710251\n",
      "epoch 60: loss is 0.12010560929775238\n",
      "epoch 61: loss is 0.12005549669265747\n",
      "epoch 62: loss is 0.12000583857297897\n",
      "epoch 63: loss is 0.1199565902352333\n",
      "epoch 64: loss is 0.11990774422883987\n",
      "epoch 65: loss is 0.11985934525728226\n",
      "epoch 66: loss is 0.11981133371591568\n",
      "epoch 67: loss is 0.11976371705532074\n",
      "epoch 68: loss is 0.11971651017665863\n",
      "epoch 69: loss is 0.11966969072818756\n",
      "epoch 70: loss is 0.11962324380874634\n",
      "epoch 71: loss is 0.11957714706659317\n",
      "epoch 72: loss is 0.11953148990869522\n",
      "epoch 73: loss is 0.11948613822460175\n",
      "epoch 74: loss is 0.11944118142127991\n",
      "epoch 75: loss is 0.11939654499292374\n",
      "epoch 76: loss is 0.11935228109359741\n",
      "epoch 77: loss is 0.11930830776691437\n",
      "epoch 78: loss is 0.11926474422216415\n",
      "epoch 79: loss is 0.11922146379947662\n",
      "epoch 80: loss is 0.11917854100465775\n",
      "epoch 81: loss is 0.11913587898015976\n",
      "epoch 82: loss is 0.11909361183643341\n",
      "epoch 83: loss is 0.11905161291360855\n",
      "epoch 84: loss is 0.11900996416807175\n",
      "epoch 85: loss is 0.11896856129169464\n",
      "epoch 86: loss is 0.11892751604318619\n",
      "epoch 87: loss is 0.11888672411441803\n",
      "epoch 88: loss is 0.11884627491235733\n",
      "epoch 89: loss is 0.11880608648061752\n",
      "epoch 90: loss is 0.118766188621521\n",
      "epoch 91: loss is 0.11872652918100357\n",
      "epoch 92: loss is 0.118687205016613\n",
      "epoch 93: loss is 0.11864811927080154\n",
      "epoch 94: loss is 0.11860935389995575\n",
      "epoch 95: loss is 0.11857084929943085\n",
      "epoch 96: loss is 0.11853259801864624\n",
      "epoch 97: loss is 0.11849461495876312\n",
      "epoch 98: loss is 0.1184568926692009\n",
      "epoch 99: loss is 0.11841943114995956\n",
      "epoch 100: loss is 0.11838218569755554\n",
      "epoch 101: loss is 0.1183452233672142\n",
      "epoch 102: loss is 0.11830848455429077\n",
      "epoch 103: loss is 0.11827200651168823\n",
      "epoch 104: loss is 0.1182357519865036\n",
      "epoch 105: loss is 0.11819973587989807\n",
      "epoch 106: loss is 0.11816393584012985\n",
      "epoch 107: loss is 0.11812836676836014\n",
      "epoch 108: loss is 0.11809300631284714\n",
      "epoch 109: loss is 0.11805792897939682\n",
      "epoch 110: loss is 0.11802300065755844\n",
      "epoch 111: loss is 0.11798832565546036\n",
      "epoch 112: loss is 0.11795385181903839\n",
      "epoch 113: loss is 0.11791963875293732\n",
      "epoch 114: loss is 0.11788556724786758\n",
      "epoch 115: loss is 0.11785179376602173\n",
      "epoch 116: loss is 0.11781816184520721\n",
      "epoch 117: loss is 0.11778473854064941\n",
      "epoch 118: loss is 0.11775150150060654\n",
      "epoch 119: loss is 0.11771848797798157\n",
      "epoch 120: loss is 0.11768565326929092\n",
      "epoch 121: loss is 0.11765304207801819\n",
      "epoch 122: loss is 0.11762057989835739\n",
      "epoch 123: loss is 0.1175883412361145\n",
      "epoch 124: loss is 0.11755628883838654\n",
      "epoch 125: loss is 0.1175243929028511\n",
      "epoch 126: loss is 0.11749273538589478\n",
      "epoch 127: loss is 0.11746122688055038\n",
      "epoch 128: loss is 0.11742991954088211\n",
      "epoch 129: loss is 0.11739875376224518\n",
      "epoch 130: loss is 0.11736779659986496\n",
      "epoch 131: loss is 0.11733698099851608\n",
      "epoch 132: loss is 0.11730632930994034\n",
      "epoch 133: loss is 0.11727579683065414\n",
      "epoch 134: loss is 0.11724548786878586\n",
      "epoch 135: loss is 0.11721528321504593\n",
      "epoch 136: loss is 0.1171853095293045\n",
      "epoch 137: loss is 0.11715548485517502\n",
      "epoch 138: loss is 0.11712578684091568\n",
      "epoch 139: loss is 0.11709567159414291\n",
      "epoch 140: loss is 0.11706576496362686\n",
      "epoch 141: loss is 0.11703597754240036\n",
      "epoch 142: loss is 0.11700636148452759\n",
      "epoch 143: loss is 0.11697690188884735\n",
      "epoch 144: loss is 0.11694757640361786\n",
      "epoch 145: loss is 0.11691844463348389\n",
      "epoch 146: loss is 0.11688943952322006\n",
      "epoch 147: loss is 0.11686066538095474\n",
      "epoch 148: loss is 0.11683192849159241\n",
      "epoch 149: loss is 0.11680340766906738\n",
      "epoch 150: loss is 0.11677497625350952\n",
      "epoch 151: loss is 0.11674671620130539\n",
      "epoch 152: loss is 0.11671857535839081\n",
      "epoch 153: loss is 0.11669059097766876\n",
      "epoch 154: loss is 0.11666271835565567\n",
      "epoch 155: loss is 0.11663505434989929\n",
      "epoch 156: loss is 0.11660751700401306\n",
      "epoch 157: loss is 0.11658008396625519\n",
      "epoch 158: loss is 0.11655278503894806\n",
      "epoch 159: loss is 0.11652559041976929\n",
      "epoch 160: loss is 0.11649855971336365\n",
      "epoch 161: loss is 0.11647161841392517\n",
      "epoch 162: loss is 0.11644486337900162\n",
      "epoch 163: loss is 0.11641820520162582\n",
      "epoch 164: loss is 0.11639174818992615\n",
      "epoch 165: loss is 0.11636534333229065\n",
      "epoch 166: loss is 0.11633908003568649\n",
      "epoch 167: loss is 0.11631293594837189\n",
      "epoch 168: loss is 0.11628691107034683\n",
      "epoch 169: loss is 0.11626101285219193\n",
      "epoch 170: loss is 0.11623521149158478\n",
      "epoch 171: loss is 0.11620958894491196\n",
      "epoch 172: loss is 0.11618408560752869\n",
      "epoch 173: loss is 0.11615870893001556\n",
      "epoch 174: loss is 0.11613339185714722\n",
      "epoch 175: loss is 0.11610820144414902\n",
      "epoch 176: loss is 0.11608315259218216\n",
      "epoch 177: loss is 0.11605818569660187\n",
      "epoch 178: loss is 0.11603335291147232\n",
      "epoch 179: loss is 0.11600865423679352\n",
      "epoch 180: loss is 0.11598410457372665\n",
      "epoch 181: loss is 0.11595959216356277\n",
      "epoch 182: loss is 0.11593519896268845\n",
      "epoch 183: loss is 0.11591091752052307\n",
      "epoch 184: loss is 0.11588675528764725\n",
      "epoch 185: loss is 0.11586267501115799\n",
      "epoch 186: loss is 0.11583872139453888\n",
      "epoch 187: loss is 0.11581495404243469\n",
      "epoch 188: loss is 0.1157911941409111\n",
      "epoch 189: loss is 0.11576757580041885\n",
      "epoch 190: loss is 0.11574399471282959\n",
      "epoch 191: loss is 0.11572059243917465\n",
      "epoch 192: loss is 0.11569724977016449\n",
      "epoch 193: loss is 0.11567401885986328\n",
      "epoch 194: loss is 0.11565094441175461\n",
      "epoch 195: loss is 0.11562788486480713\n",
      "epoch 196: loss is 0.11560498178005219\n",
      "epoch 197: loss is 0.11558210849761963\n",
      "epoch 198: loss is 0.1155594140291214\n",
      "epoch 199: loss is 0.11553674191236496\n",
      "epoch 200: loss is 0.11551423370838165\n",
      "epoch 201: loss is 0.11549180746078491\n",
      "epoch 202: loss is 0.11546944081783295\n",
      "epoch 203: loss is 0.11544720828533173\n",
      "epoch 204: loss is 0.1154250055551529\n",
      "epoch 205: loss is 0.1154029443860054\n",
      "epoch 206: loss is 0.11538093537092209\n",
      "epoch 207: loss is 0.1153591126203537\n",
      "epoch 208: loss is 0.1153373271226883\n",
      "epoch 209: loss is 0.11531559377908707\n",
      "epoch 210: loss is 0.11529397964477539\n",
      "epoch 211: loss is 0.1152724102139473\n",
      "epoch 212: loss is 0.11525095254182816\n",
      "epoch 213: loss is 0.11522965878248215\n",
      "epoch 214: loss is 0.11520840227603912\n",
      "epoch 215: loss is 0.11518719047307968\n",
      "epoch 216: loss is 0.115166075527668\n",
      "epoch 217: loss is 0.11514506489038467\n",
      "epoch 218: loss is 0.11512412875890732\n",
      "epoch 219: loss is 0.1151033267378807\n",
      "epoch 220: loss is 0.11508255451917648\n",
      "epoch 221: loss is 0.11506185680627823\n",
      "epoch 222: loss is 0.11504127085208893\n",
      "epoch 223: loss is 0.11502072960138321\n",
      "epoch 224: loss is 0.11500027775764465\n",
      "epoch 225: loss is 0.11497996747493744\n",
      "epoch 226: loss is 0.1149597018957138\n",
      "epoch 227: loss is 0.1149396151304245\n",
      "epoch 228: loss is 0.11491964757442474\n",
      "epoch 229: loss is 0.11489980667829514\n",
      "epoch 230: loss is 0.11487998813390732\n",
      "epoch 231: loss is 0.114860400557518\n",
      "epoch 232: loss is 0.11484076082706451\n",
      "epoch 233: loss is 0.11482119560241699\n",
      "epoch 234: loss is 0.11480174213647842\n",
      "epoch 235: loss is 0.11478234082460403\n",
      "epoch 236: loss is 0.11476308107376099\n",
      "epoch 237: loss is 0.11474385112524033\n",
      "epoch 238: loss is 0.11472469568252563\n",
      "epoch 239: loss is 0.11470570415258408\n",
      "epoch 240: loss is 0.11468672007322311\n",
      "epoch 241: loss is 0.1146678626537323\n",
      "epoch 242: loss is 0.11464915424585342\n",
      "epoch 243: loss is 0.11463039368391037\n",
      "epoch 244: loss is 0.11461176723241806\n",
      "epoch 245: loss is 0.11459314823150635\n",
      "epoch 246: loss is 0.11457464843988419\n",
      "epoch 247: loss is 0.11455626040697098\n",
      "epoch 248: loss is 0.11453788727521896\n",
      "epoch 249: loss is 0.11451959609985352\n",
      "epoch 250: loss is 0.11450131982564926\n",
      "epoch 251: loss is 0.11448319256305695\n",
      "epoch 252: loss is 0.11446513235569\n",
      "epoch 253: loss is 0.11444710195064545\n",
      "epoch 254: loss is 0.11442913115024567\n",
      "epoch 255: loss is 0.11441120505332947\n",
      "epoch 256: loss is 0.11439337581396103\n",
      "epoch 257: loss is 0.11437564343214035\n",
      "epoch 258: loss is 0.11435795575380325\n",
      "epoch 259: loss is 0.11434032768011093\n",
      "epoch 260: loss is 0.114322729408741\n",
      "epoch 261: loss is 0.11430520564317703\n",
      "epoch 262: loss is 0.11428782343864441\n",
      "epoch 263: loss is 0.11427048593759537\n",
      "epoch 264: loss is 0.11425319314002991\n",
      "epoch 265: loss is 0.11423592269420624\n",
      "epoch 266: loss is 0.11421873420476913\n",
      "epoch 267: loss is 0.11420169472694397\n",
      "epoch 268: loss is 0.11418461054563522\n",
      "epoch 269: loss is 0.11416764557361603\n",
      "epoch 270: loss is 0.11415068060159683\n",
      "epoch 271: loss is 0.1141338050365448\n",
      "epoch 272: loss is 0.1141171008348465\n",
      "epoch 273: loss is 0.11410029977560043\n",
      "epoch 274: loss is 0.11408361047506332\n",
      "epoch 275: loss is 0.1140669509768486\n",
      "epoch 276: loss is 0.11405037343502045\n",
      "epoch 277: loss is 0.11403397470712662\n",
      "epoch 278: loss is 0.11401752382516861\n",
      "epoch 279: loss is 0.11400114744901657\n",
      "epoch 280: loss is 0.11398481577634811\n",
      "epoch 281: loss is 0.1139686182141304\n",
      "epoch 282: loss is 0.11395244300365448\n",
      "epoch 283: loss is 0.11393626779317856\n",
      "epoch 284: loss is 0.11392015963792801\n",
      "epoch 285: loss is 0.11390414834022522\n",
      "epoch 286: loss is 0.11388823390007019\n",
      "epoch 287: loss is 0.11387229710817337\n",
      "epoch 288: loss is 0.11385641247034073\n",
      "epoch 289: loss is 0.11384055763483047\n",
      "epoch 290: loss is 0.11382490396499634\n",
      "epoch 291: loss is 0.11380919069051743\n",
      "epoch 292: loss is 0.1137934997677803\n",
      "epoch 293: loss is 0.11377789080142975\n",
      "epoch 294: loss is 0.11376233398914337\n",
      "epoch 295: loss is 0.11374691873788834\n",
      "epoch 296: loss is 0.11373146623373032\n",
      "epoch 297: loss is 0.1137160211801529\n",
      "epoch 298: loss is 0.11370065808296204\n",
      "epoch 299: loss is 0.1136854737997055\n",
      "epoch 300: loss is 0.11367017775774002\n",
      "epoch 301: loss is 0.11365500837564468\n",
      "epoch 302: loss is 0.11363983154296875\n",
      "epoch 303: loss is 0.11362478137016296\n",
      "epoch 304: loss is 0.11360978335142136\n",
      "epoch 305: loss is 0.11359474062919617\n",
      "epoch 306: loss is 0.11357980221509933\n",
      "epoch 307: loss is 0.11356492340564728\n",
      "epoch 308: loss is 0.1135500967502594\n",
      "epoch 309: loss is 0.11353529989719391\n",
      "epoch 310: loss is 0.11352049559354782\n",
      "epoch 311: loss is 0.11350581049919128\n",
      "epoch 312: loss is 0.1134912446141243\n",
      "epoch 313: loss is 0.11347658187150955\n",
      "epoch 314: loss is 0.11346198618412018\n",
      "epoch 315: loss is 0.11344750225543976\n",
      "epoch 316: loss is 0.11343308538198471\n",
      "epoch 317: loss is 0.11341866105794907\n",
      "epoch 318: loss is 0.11340425163507462\n",
      "epoch 319: loss is 0.11338989436626434\n",
      "epoch 320: loss is 0.1133757010102272\n",
      "epoch 321: loss is 0.1133614033460617\n",
      "epoch 322: loss is 0.11334716528654099\n",
      "epoch 323: loss is 0.11333301663398743\n",
      "epoch 324: loss is 0.11331899464130402\n",
      "epoch 325: loss is 0.11330490559339523\n",
      "epoch 326: loss is 0.11329086124897003\n",
      "epoch 327: loss is 0.11327686160802841\n",
      "epoch 328: loss is 0.11326300352811813\n",
      "epoch 329: loss is 0.11324909329414368\n",
      "epoch 330: loss is 0.11323521286249161\n",
      "epoch 331: loss is 0.1132214143872261\n",
      "epoch 332: loss is 0.11320772767066956\n",
      "epoch 333: loss is 0.11319393664598465\n",
      "epoch 334: loss is 0.1131802350282669\n",
      "epoch 335: loss is 0.11316663771867752\n",
      "epoch 336: loss is 0.11315308511257172\n",
      "epoch 337: loss is 0.11313952505588531\n",
      "epoch 338: loss is 0.11312595754861832\n",
      "epoch 339: loss is 0.11311250925064087\n",
      "epoch 340: loss is 0.11309915035963058\n",
      "epoch 341: loss is 0.11308570206165314\n",
      "epoch 342: loss is 0.11307229846715927\n",
      "epoch 343: loss is 0.11305909603834152\n",
      "epoch 344: loss is 0.11304580420255661\n",
      "epoch 345: loss is 0.11303254216909409\n",
      "epoch 346: loss is 0.11301931738853455\n",
      "epoch 347: loss is 0.11300627142190933\n",
      "epoch 348: loss is 0.11299310624599457\n",
      "epoch 349: loss is 0.11298005282878876\n",
      "epoch 350: loss is 0.11296699196100235\n",
      "epoch 351: loss is 0.11295410245656967\n",
      "epoch 352: loss is 0.11294113844633102\n",
      "epoch 353: loss is 0.11292814463376999\n",
      "epoch 354: loss is 0.11291537433862686\n",
      "epoch 355: loss is 0.11290255188941956\n",
      "epoch 356: loss is 0.11288972198963165\n",
      "epoch 357: loss is 0.11287693679332733\n",
      "epoch 358: loss is 0.11286433786153793\n",
      "epoch 359: loss is 0.11285161972045898\n",
      "epoch 360: loss is 0.11283893138170242\n",
      "epoch 361: loss is 0.11282632499933243\n",
      "epoch 362: loss is 0.11281383037567139\n",
      "epoch 363: loss is 0.11280125379562378\n",
      "epoch 364: loss is 0.11278874427080154\n",
      "epoch 365: loss is 0.11277636885643005\n",
      "epoch 366: loss is 0.11276398599147797\n",
      "epoch 367: loss is 0.11275158077478409\n",
      "epoch 368: loss is 0.11273924261331558\n",
      "epoch 369: loss is 0.11272706836462021\n",
      "epoch 370: loss is 0.11271476000547409\n",
      "epoch 371: loss is 0.11270249634981155\n",
      "epoch 372: loss is 0.11269038170576096\n",
      "epoch 373: loss is 0.11267824470996857\n",
      "epoch 374: loss is 0.11266607791185379\n",
      "epoch 375: loss is 0.11265400052070618\n",
      "epoch 376: loss is 0.11264202743768692\n",
      "epoch 377: loss is 0.1126299798488617\n",
      "epoch 378: loss is 0.11261796206235886\n",
      "epoch 379: loss is 0.11260608583688736\n",
      "epoch 380: loss is 0.11259417980909348\n",
      "epoch 381: loss is 0.11258230358362198\n",
      "epoch 382: loss is 0.11257045716047287\n",
      "epoch 383: loss is 0.11255873739719391\n",
      "epoch 384: loss is 0.11254698038101196\n",
      "epoch 385: loss is 0.11253520846366882\n",
      "epoch 386: loss is 0.11252359300851822\n",
      "epoch 387: loss is 0.11251194030046463\n",
      "epoch 388: loss is 0.11250028759241104\n",
      "epoch 389: loss is 0.11248868703842163\n",
      "epoch 390: loss is 0.11247722059488297\n",
      "epoch 391: loss is 0.11246564984321594\n",
      "epoch 392: loss is 0.1124541237950325\n",
      "epoch 393: loss is 0.11244279891252518\n",
      "epoch 394: loss is 0.1124313548207283\n",
      "epoch 395: loss is 0.11241991072893143\n",
      "epoch 396: loss is 0.11240863800048828\n",
      "epoch 397: loss is 0.11239732801914215\n",
      "epoch 398: loss is 0.11238601058721542\n",
      "epoch 399: loss is 0.11237484216690063\n",
      "epoch 400: loss is 0.11236372590065002\n",
      "epoch 401: loss is 0.11235250532627106\n",
      "epoch 402: loss is 0.11234136670827866\n",
      "epoch 403: loss is 0.11233039200305939\n",
      "epoch 404: loss is 0.11231926083564758\n",
      "epoch 405: loss is 0.11230820417404175\n",
      "epoch 406: loss is 0.11229729652404785\n",
      "epoch 407: loss is 0.11228629946708679\n",
      "epoch 408: loss is 0.11227531731128693\n",
      "epoch 409: loss is 0.11226443201303482\n",
      "epoch 410: loss is 0.1122535765171051\n",
      "epoch 411: loss is 0.112242691218853\n",
      "epoch 412: loss is 0.11223185062408447\n",
      "epoch 413: loss is 0.1122211292386055\n",
      "epoch 414: loss is 0.11221029609441757\n",
      "epoch 415: loss is 0.11219951510429382\n",
      "epoch 416: loss is 0.112188920378685\n",
      "epoch 417: loss is 0.11217816174030304\n",
      "epoch 418: loss is 0.11216748505830765\n",
      "epoch 419: loss is 0.112156942486763\n",
      "epoch 420: loss is 0.11214631795883179\n",
      "epoch 421: loss is 0.11213570833206177\n",
      "epoch 422: loss is 0.1121252104640007\n",
      "epoch 423: loss is 0.11211466044187546\n",
      "epoch 424: loss is 0.11210410296916962\n",
      "epoch 425: loss is 0.11209366470575333\n",
      "epoch 426: loss is 0.11208319664001465\n",
      "epoch 427: loss is 0.11207271367311478\n",
      "epoch 428: loss is 0.11206241697072983\n",
      "epoch 429: loss is 0.11205215007066727\n",
      "epoch 430: loss is 0.11204181611537933\n",
      "epoch 431: loss is 0.11203158646821976\n",
      "epoch 432: loss is 0.11202143877744675\n",
      "epoch 433: loss is 0.11201118677854538\n",
      "epoch 434: loss is 0.11200098693370819\n",
      "epoch 435: loss is 0.11199092864990234\n",
      "epoch 436: loss is 0.11198075860738754\n",
      "epoch 437: loss is 0.11197061836719513\n",
      "epoch 438: loss is 0.11196064203977585\n",
      "epoch 439: loss is 0.11195056140422821\n",
      "epoch 440: loss is 0.11194050312042236\n",
      "epoch 441: loss is 0.11193060874938965\n",
      "epoch 442: loss is 0.11192058026790619\n",
      "epoch 443: loss is 0.1119106188416481\n",
      "epoch 444: loss is 0.11190079152584076\n",
      "epoch 445: loss is 0.11189085245132446\n",
      "epoch 446: loss is 0.11188094317913055\n",
      "epoch 447: loss is 0.11187119036912918\n",
      "epoch 448: loss is 0.11186134815216064\n",
      "epoch 449: loss is 0.11185149103403091\n",
      "epoch 450: loss is 0.1118418276309967\n",
      "epoch 451: loss is 0.11183204501867294\n",
      "epoch 452: loss is 0.11182226240634918\n",
      "epoch 453: loss is 0.11181265115737915\n",
      "epoch 454: loss is 0.11180293560028076\n",
      "epoch 455: loss is 0.11179323494434357\n",
      "epoch 456: loss is 0.1117837131023407\n",
      "epoch 457: loss is 0.11177405714988708\n",
      "epoch 458: loss is 0.11176442354917526\n",
      "epoch 459: loss is 0.11175493896007538\n",
      "epoch 460: loss is 0.11174537986516953\n",
      "epoch 461: loss is 0.11173580586910248\n",
      "epoch 462: loss is 0.11172641068696976\n",
      "epoch 463: loss is 0.11171691119670868\n",
      "epoch 464: loss is 0.111707404255867\n",
      "epoch 465: loss is 0.11169808357954025\n",
      "epoch 466: loss is 0.11168864369392395\n",
      "epoch 467: loss is 0.11167921125888824\n",
      "epoch 468: loss is 0.11166995018720627\n",
      "epoch 469: loss is 0.11166057735681534\n",
      "epoch 470: loss is 0.11165119707584381\n",
      "epoch 471: loss is 0.1116420179605484\n",
      "epoch 472: loss is 0.11163271963596344\n",
      "epoch 473: loss is 0.11162341386079788\n",
      "epoch 474: loss is 0.11161429435014725\n",
      "epoch 475: loss is 0.11160504817962646\n",
      "epoch 476: loss is 0.11159583181142807\n",
      "epoch 477: loss is 0.111586794257164\n",
      "epoch 478: loss is 0.11157765239477158\n",
      "epoch 479: loss is 0.11156868189573288\n",
      "epoch 480: loss is 0.11155979335308075\n",
      "epoch 481: loss is 0.11155077815055847\n",
      "epoch 482: loss is 0.11154188215732574\n",
      "epoch 483: loss is 0.11153305321931839\n",
      "epoch 484: loss is 0.11152411997318268\n",
      "epoch 485: loss is 0.11151529848575592\n",
      "epoch 486: loss is 0.11150648444890976\n",
      "epoch 487: loss is 0.11149758845567703\n",
      "epoch 488: loss is 0.11148885637521744\n",
      "epoch 489: loss is 0.11148007959127426\n",
      "epoch 490: loss is 0.11147116869688034\n",
      "epoch 491: loss is 0.11146242171525955\n",
      "epoch 492: loss is 0.111453577876091\n",
      "epoch 493: loss is 0.11144473403692245\n",
      "epoch 494: loss is 0.11143604665994644\n",
      "epoch 495: loss is 0.11142726987600327\n",
      "epoch 496: loss is 0.11141848564147949\n",
      "epoch 497: loss is 0.11140987277030945\n",
      "epoch 498: loss is 0.11140111088752747\n",
      "epoch 499: loss is 0.11139242351055145\n",
      "epoch 500: loss is 0.1113838255405426\n",
      "epoch 501: loss is 0.1113751158118248\n",
      "epoch 502: loss is 0.11136656999588013\n",
      "epoch 503: loss is 0.11135795712471008\n",
      "epoch 504: loss is 0.11134929209947586\n",
      "epoch 505: loss is 0.11134086549282074\n",
      "epoch 506: loss is 0.11133230477571487\n",
      "epoch 507: loss is 0.11132373660802841\n",
      "epoch 508: loss is 0.11131539940834045\n",
      "epoch 509: loss is 0.11130683869123459\n",
      "epoch 510: loss is 0.11129835993051529\n",
      "epoch 511: loss is 0.11129004508256912\n",
      "epoch 512: loss is 0.11128154397010803\n",
      "epoch 513: loss is 0.11127318441867828\n",
      "epoch 514: loss is 0.11126484721899033\n",
      "epoch 515: loss is 0.1112564355134964\n",
      "epoch 516: loss is 0.1112481877207756\n",
      "epoch 517: loss is 0.11123981326818466\n",
      "epoch 518: loss is 0.1112314760684967\n",
      "epoch 519: loss is 0.11122331768274307\n",
      "epoch 520: loss is 0.11121498048305511\n",
      "epoch 521: loss is 0.11120671033859253\n",
      "epoch 522: loss is 0.11119857430458069\n",
      "epoch 523: loss is 0.11119028180837631\n",
      "epoch 524: loss is 0.11118212342262268\n",
      "epoch 525: loss is 0.11117397993803024\n",
      "epoch 526: loss is 0.11116573959589005\n",
      "epoch 527: loss is 0.11115769296884537\n",
      "epoch 528: loss is 0.11114953458309174\n",
      "epoch 529: loss is 0.1111413985490799\n",
      "epoch 530: loss is 0.1111334040760994\n",
      "epoch 531: loss is 0.11112524569034576\n",
      "epoch 532: loss is 0.11111725121736526\n",
      "epoch 533: loss is 0.11110924184322357\n",
      "epoch 534: loss is 0.11110113561153412\n",
      "epoch 535: loss is 0.11109323799610138\n",
      "epoch 536: loss is 0.11108524352312088\n",
      "epoch 537: loss is 0.1110772117972374\n",
      "epoch 538: loss is 0.11106938868761063\n",
      "epoch 539: loss is 0.11106135696172714\n",
      "epoch 540: loss is 0.1110534816980362\n",
      "epoch 541: loss is 0.11104560643434525\n",
      "epoch 542: loss is 0.11103767156600952\n",
      "epoch 543: loss is 0.11102987825870514\n",
      "epoch 544: loss is 0.111021988093853\n",
      "epoch 545: loss is 0.11101415008306503\n",
      "epoch 546: loss is 0.11100641638040543\n",
      "epoch 547: loss is 0.11099851131439209\n",
      "epoch 548: loss is 0.11099080741405487\n",
      "epoch 549: loss is 0.11098303645849228\n",
      "epoch 550: loss is 0.11097520589828491\n",
      "epoch 551: loss is 0.11096756160259247\n",
      "epoch 552: loss is 0.11095982044935226\n",
      "epoch 553: loss is 0.11095209419727325\n",
      "epoch 554: loss is 0.110944464802742\n",
      "epoch 555: loss is 0.11093676090240479\n",
      "epoch 556: loss is 0.11092916131019592\n",
      "epoch 557: loss is 0.11092152446508408\n",
      "epoch 558: loss is 0.11091379821300507\n",
      "epoch 559: loss is 0.11090634018182755\n",
      "epoch 560: loss is 0.11089868098497391\n",
      "epoch 561: loss is 0.11089110374450684\n",
      "epoch 562: loss is 0.11088359355926514\n",
      "epoch 563: loss is 0.11087597161531448\n",
      "epoch 564: loss is 0.11086855083703995\n",
      "epoch 565: loss is 0.11086098849773407\n",
      "epoch 566: loss is 0.11085347831249237\n",
      "epoch 567: loss is 0.11084604263305664\n",
      "epoch 568: loss is 0.11083852499723434\n",
      "epoch 569: loss is 0.1108311340212822\n",
      "epoch 570: loss is 0.11082365363836288\n",
      "epoch 571: loss is 0.11081616580486298\n",
      "epoch 572: loss is 0.11080891638994217\n",
      "epoch 573: loss is 0.11080144345760345\n",
      "epoch 574: loss is 0.1107940524816513\n",
      "epoch 575: loss is 0.11078673601150513\n",
      "epoch 576: loss is 0.11077931523323059\n",
      "epoch 577: loss is 0.11077208071947098\n",
      "epoch 578: loss is 0.11076469719409943\n",
      "epoch 579: loss is 0.11075739562511444\n",
      "epoch 580: loss is 0.11075016111135483\n",
      "epoch 581: loss is 0.11074277758598328\n",
      "epoch 582: loss is 0.11073564738035202\n",
      "epoch 583: loss is 0.11072831600904465\n",
      "epoch 584: loss is 0.11072107404470444\n",
      "epoch 585: loss is 0.1107138991355896\n",
      "epoch 586: loss is 0.1107066422700882\n",
      "epoch 587: loss is 0.11069952696561813\n",
      "epoch 588: loss is 0.11069230735301971\n",
      "epoch 589: loss is 0.11068514734506607\n",
      "epoch 590: loss is 0.1106780543923378\n",
      "epoch 591: loss is 0.11067082732915878\n",
      "epoch 592: loss is 0.11066379398107529\n",
      "epoch 593: loss is 0.11065667867660522\n",
      "epoch 594: loss is 0.11064953356981277\n",
      "epoch 595: loss is 0.11064255982637405\n",
      "epoch 596: loss is 0.11063539981842041\n",
      "epoch 597: loss is 0.11062843352556229\n",
      "epoch 598: loss is 0.11062135547399521\n",
      "epoch 599: loss is 0.11061431467533112\n",
      "epoch 600: loss is 0.11060736328363419\n",
      "epoch 601: loss is 0.11060028523206711\n",
      "epoch 602: loss is 0.11059340089559555\n",
      "epoch 603: loss is 0.11058640480041504\n",
      "epoch 604: loss is 0.11057940125465393\n",
      "epoch 605: loss is 0.11057250201702118\n",
      "epoch 606: loss is 0.11056552082300186\n",
      "epoch 607: loss is 0.11055868864059448\n",
      "epoch 608: loss is 0.11055171489715576\n",
      "epoch 609: loss is 0.11054481565952301\n",
      "epoch 610: loss is 0.11053799092769623\n",
      "epoch 611: loss is 0.11053106188774109\n",
      "epoch 612: loss is 0.11052428930997849\n",
      "epoch 613: loss is 0.11051740497350693\n",
      "epoch 614: loss is 0.11051055043935776\n",
      "epoch 615: loss is 0.11050378531217575\n",
      "epoch 616: loss is 0.11049692332744598\n",
      "epoch 617: loss is 0.11049024015665054\n",
      "epoch 618: loss is 0.11048335582017899\n",
      "epoch 619: loss is 0.11047663539648056\n",
      "epoch 620: loss is 0.11046992987394333\n",
      "epoch 621: loss is 0.11046310514211655\n",
      "epoch 622: loss is 0.11045649647712708\n",
      "epoch 623: loss is 0.11044970154762268\n",
      "epoch 624: loss is 0.11044303327798843\n",
      "epoch 625: loss is 0.1104363426566124\n",
      "epoch 626: loss is 0.11042960733175278\n",
      "epoch 627: loss is 0.11042303591966629\n",
      "epoch 628: loss is 0.11041627824306488\n",
      "epoch 629: loss is 0.11040972173213959\n",
      "epoch 630: loss is 0.11040309071540833\n",
      "epoch 631: loss is 0.11039643734693527\n",
      "epoch 632: loss is 0.11038987338542938\n",
      "epoch 633: loss is 0.11038318276405334\n",
      "epoch 634: loss is 0.1103767454624176\n",
      "epoch 635: loss is 0.11037008464336395\n",
      "epoch 636: loss is 0.11036353558301926\n",
      "epoch 637: loss is 0.11035700142383575\n",
      "epoch 638: loss is 0.11035038530826569\n",
      "epoch 639: loss is 0.11034400016069412\n",
      "epoch 640: loss is 0.11033739149570465\n",
      "epoch 641: loss is 0.11033093929290771\n",
      "epoch 642: loss is 0.11032439768314362\n",
      "epoch 643: loss is 0.1103179082274437\n",
      "epoch 644: loss is 0.11031153798103333\n",
      "epoch 645: loss is 0.11030498147010803\n",
      "epoch 646: loss is 0.11029866337776184\n",
      "epoch 647: loss is 0.11029217392206192\n",
      "epoch 648: loss is 0.11028577387332916\n",
      "epoch 649: loss is 0.110279381275177\n",
      "epoch 650: loss is 0.11027292907238007\n",
      "epoch 651: loss is 0.11026664078235626\n",
      "epoch 652: loss is 0.11026018112897873\n",
      "epoch 653: loss is 0.11025385558605194\n",
      "epoch 654: loss is 0.11024751514196396\n",
      "epoch 655: loss is 0.11024116724729538\n",
      "epoch 656: loss is 0.11023486405611038\n",
      "epoch 657: loss is 0.11022844910621643\n",
      "epoch 658: loss is 0.11022227257490158\n",
      "epoch 659: loss is 0.11021587997674942\n",
      "epoch 660: loss is 0.11020965129137039\n",
      "epoch 661: loss is 0.11020337045192719\n",
      "epoch 662: loss is 0.11019705981016159\n",
      "epoch 663: loss is 0.11019085347652435\n",
      "epoch 664: loss is 0.11018454283475876\n",
      "epoch 665: loss is 0.11017842590808868\n",
      "epoch 666: loss is 0.1101721003651619\n",
      "epoch 667: loss is 0.11016596108675003\n",
      "epoch 668: loss is 0.11015976220369339\n",
      "epoch 669: loss is 0.11015351116657257\n",
      "epoch 670: loss is 0.11014744639396667\n",
      "epoch 671: loss is 0.11014119535684586\n",
      "epoch 672: loss is 0.11013512313365936\n",
      "epoch 673: loss is 0.11012892425060272\n",
      "epoch 674: loss is 0.11012280732393265\n",
      "epoch 675: loss is 0.11011672019958496\n",
      "epoch 676: loss is 0.1101105585694313\n",
      "epoch 677: loss is 0.11010453850030899\n",
      "epoch 678: loss is 0.11009836196899414\n",
      "epoch 679: loss is 0.11009237170219421\n",
      "epoch 680: loss is 0.11008623242378235\n",
      "epoch 681: loss is 0.11008020490407944\n",
      "epoch 682: loss is 0.11007416993379593\n",
      "epoch 683: loss is 0.11006806045770645\n",
      "epoch 684: loss is 0.1100621148943901\n",
      "epoch 685: loss is 0.11005597561597824\n",
      "epoch 686: loss is 0.11005009710788727\n",
      "epoch 687: loss is 0.1100439727306366\n",
      "epoch 688: loss is 0.11003803461790085\n",
      "epoch 689: loss is 0.11003205925226212\n",
      "epoch 690: loss is 0.11002601683139801\n",
      "epoch 691: loss is 0.11002016812562943\n",
      "epoch 692: loss is 0.11001411825418472\n",
      "epoch 693: loss is 0.11000830680131912\n",
      "epoch 694: loss is 0.11000227928161621\n",
      "epoch 695: loss is 0.10999643057584763\n",
      "epoch 696: loss is 0.10999047011137009\n",
      "epoch 697: loss is 0.10998456180095673\n",
      "epoch 698: loss is 0.10997872054576874\n",
      "epoch 699: loss is 0.1099727600812912\n",
      "epoch 700: loss is 0.10996700078248978\n",
      "epoch 701: loss is 0.10996104031801224\n",
      "epoch 702: loss is 0.10995529592037201\n",
      "epoch 703: loss is 0.10994938015937805\n",
      "epoch 704: loss is 0.10994355380535126\n",
      "epoch 705: loss is 0.10993775725364685\n",
      "epoch 706: loss is 0.10993187874555588\n",
      "epoch 707: loss is 0.10992613434791565\n",
      "epoch 708: loss is 0.10992022603750229\n",
      "epoch 709: loss is 0.10991459339857101\n",
      "epoch 710: loss is 0.10990871489048004\n",
      "epoch 711: loss is 0.1099030002951622\n",
      "epoch 712: loss is 0.1098972037434578\n",
      "epoch 713: loss is 0.10989143699407578\n",
      "epoch 714: loss is 0.10988570749759674\n",
      "epoch 715: loss is 0.10987991839647293\n",
      "epoch 716: loss is 0.10987430065870285\n",
      "epoch 717: loss is 0.10986848175525665\n",
      "epoch 718: loss is 0.10986284911632538\n",
      "epoch 719: loss is 0.10985710471868515\n",
      "epoch 720: loss is 0.10985143482685089\n",
      "epoch 721: loss is 0.10984575748443604\n",
      "epoch 722: loss is 0.10984005779027939\n",
      "epoch 723: loss is 0.1098344624042511\n",
      "epoch 724: loss is 0.10982868820428848\n",
      "epoch 725: loss is 0.10982313752174377\n",
      "epoch 726: loss is 0.10981739312410355\n",
      "epoch 727: loss is 0.10981184244155884\n",
      "epoch 728: loss is 0.109806127846241\n",
      "epoch 729: loss is 0.10980057716369629\n",
      "epoch 730: loss is 0.10979493707418442\n",
      "epoch 731: loss is 0.10978928953409195\n",
      "epoch 732: loss is 0.10978373140096664\n",
      "epoch 733: loss is 0.10977807641029358\n",
      "epoch 734: loss is 0.10977258533239365\n",
      "epoch 735: loss is 0.10976690798997879\n",
      "epoch 736: loss is 0.10976145416498184\n",
      "epoch 737: loss is 0.10975582897663116\n",
      "epoch 738: loss is 0.10975030809640884\n",
      "epoch 739: loss is 0.10974474996328354\n",
      "epoch 740: loss is 0.10973922908306122\n",
      "epoch 741: loss is 0.10973375290632248\n",
      "epoch 742: loss is 0.10972816497087479\n",
      "epoch 743: loss is 0.10972277075052261\n",
      "epoch 744: loss is 0.10971713066101074\n",
      "epoch 745: loss is 0.10971178114414215\n",
      "epoch 746: loss is 0.10970619320869446\n",
      "epoch 747: loss is 0.10970082879066467\n",
      "epoch 748: loss is 0.10969528555870056\n",
      "epoch 749: loss is 0.1096898689866066\n",
      "epoch 750: loss is 0.10968442261219025\n",
      "epoch 751: loss is 0.10967894643545151\n",
      "epoch 752: loss is 0.10967354476451874\n",
      "epoch 753: loss is 0.10966806858778\n",
      "epoch 754: loss is 0.1096627414226532\n",
      "epoch 755: loss is 0.10965722054243088\n",
      "epoch 756: loss is 0.10965193808078766\n",
      "epoch 757: loss is 0.10964644700288773\n",
      "epoch 758: loss is 0.10964111238718033\n",
      "epoch 759: loss is 0.10963568091392517\n",
      "epoch 760: loss is 0.10963033139705658\n",
      "epoch 761: loss is 0.10962500423192978\n",
      "epoch 762: loss is 0.10961960256099701\n",
      "epoch 763: loss is 0.10961436480283737\n",
      "epoch 764: loss is 0.1096089780330658\n",
      "epoch 765: loss is 0.10960379242897034\n",
      "epoch 766: loss is 0.10959834605455399\n",
      "epoch 767: loss is 0.10959324985742569\n",
      "epoch 768: loss is 0.10958785563707352\n",
      "epoch 769: loss is 0.10958267748355865\n",
      "epoch 770: loss is 0.10957735031843185\n",
      "epoch 771: loss is 0.1095721647143364\n",
      "epoch 772: loss is 0.10956686735153198\n",
      "epoch 773: loss is 0.10956163704395294\n",
      "epoch 774: loss is 0.10955644398927689\n",
      "epoch 775: loss is 0.10955118387937546\n",
      "epoch 776: loss is 0.10954604297876358\n",
      "epoch 777: loss is 0.10954073816537857\n",
      "epoch 778: loss is 0.10953566431999207\n",
      "epoch 779: loss is 0.10953032225370407\n",
      "epoch 780: loss is 0.10952527076005936\n",
      "epoch 781: loss is 0.10952000319957733\n",
      "epoch 782: loss is 0.10951491445302963\n",
      "epoch 783: loss is 0.10950969159603119\n",
      "epoch 784: loss is 0.1095045879483223\n",
      "epoch 785: loss is 0.10949943214654922\n",
      "epoch 786: loss is 0.10949430614709854\n",
      "epoch 787: loss is 0.10948923230171204\n",
      "epoch 788: loss is 0.10948406904935837\n",
      "epoch 789: loss is 0.10947903990745544\n",
      "epoch 790: loss is 0.10947386920452118\n",
      "epoch 791: loss is 0.10946891456842422\n",
      "epoch 792: loss is 0.10946371406316757\n",
      "epoch 793: loss is 0.10945872962474823\n",
      "epoch 794: loss is 0.10945359617471695\n",
      "epoch 795: loss is 0.10944860428571701\n",
      "epoch 796: loss is 0.10944349318742752\n",
      "epoch 797: loss is 0.10943852365016937\n",
      "epoch 798: loss is 0.10943345725536346\n",
      "epoch 799: loss is 0.10942842066287994\n",
      "epoch 800: loss is 0.10942341387271881\n",
      "epoch 801: loss is 0.1094183623790741\n",
      "epoch 802: loss is 0.10941340774297714\n",
      "epoch 803: loss is 0.10940830409526825\n",
      "epoch 804: loss is 0.10940343141555786\n",
      "epoch 805: loss is 0.10939832031726837\n",
      "epoch 806: loss is 0.10939349234104156\n",
      "epoch 807: loss is 0.10938838124275208\n",
      "epoch 808: loss is 0.10938354581594467\n",
      "epoch 809: loss is 0.10937847197055817\n",
      "epoch 810: loss is 0.10937356948852539\n",
      "epoch 811: loss is 0.10936858505010605\n",
      "epoch 812: loss is 0.10936366766691208\n",
      "epoch 813: loss is 0.10935871303081512\n",
      "epoch 814: loss is 0.10935379564762115\n",
      "epoch 815: loss is 0.10934890061616898\n",
      "epoch 816: loss is 0.10934394598007202\n",
      "epoch 817: loss is 0.10933911055326462\n",
      "epoch 818: loss is 0.10933414101600647\n",
      "epoch 819: loss is 0.10932933539152145\n",
      "epoch 820: loss is 0.10932434350252151\n",
      "epoch 821: loss is 0.10931962728500366\n",
      "epoch 822: loss is 0.10931462049484253\n",
      "epoch 823: loss is 0.10930986702442169\n",
      "epoch 824: loss is 0.10930491238832474\n",
      "epoch 825: loss is 0.1093001589179039\n",
      "epoch 826: loss is 0.10929521173238754\n",
      "epoch 827: loss is 0.1092904582619667\n",
      "epoch 828: loss is 0.10928557813167572\n",
      "epoch 829: loss is 0.1092807799577713\n",
      "epoch 830: loss is 0.1092759445309639\n",
      "epoch 831: loss is 0.10927116870880127\n",
      "epoch 832: loss is 0.10926631093025208\n",
      "epoch 833: loss is 0.10926149785518646\n",
      "epoch 834: loss is 0.10925674438476562\n",
      "epoch 835: loss is 0.10925192385911942\n",
      "epoch 836: loss is 0.10924717038869858\n",
      "epoch 837: loss is 0.10924234241247177\n",
      "epoch 838: loss is 0.10923764854669571\n",
      "epoch 839: loss is 0.10923277586698532\n",
      "epoch 840: loss is 0.10922811925411224\n",
      "epoch 841: loss is 0.10922326147556305\n",
      "epoch 842: loss is 0.10921864211559296\n",
      "epoch 843: loss is 0.10921377688646317\n",
      "epoch 844: loss is 0.10920913517475128\n",
      "epoch 845: loss is 0.10920432209968567\n",
      "epoch 846: loss is 0.10919967293739319\n",
      "epoch 847: loss is 0.10919488221406937\n",
      "epoch 848: loss is 0.10919025540351868\n",
      "epoch 849: loss is 0.10918549448251724\n",
      "epoch 850: loss is 0.10918081551790237\n",
      "epoch 851: loss is 0.10917612165212631\n",
      "epoch 852: loss is 0.10917143523693085\n",
      "epoch 853: loss is 0.10916674882173538\n",
      "epoch 854: loss is 0.10916204005479813\n",
      "epoch 855: loss is 0.10915741324424744\n",
      "epoch 856: loss is 0.10915270447731018\n",
      "epoch 857: loss is 0.10914810001850128\n",
      "epoch 858: loss is 0.10914341360330582\n",
      "epoch 859: loss is 0.10913882404565811\n",
      "epoch 860: loss is 0.10913409292697906\n",
      "epoch 861: loss is 0.10912956297397614\n",
      "epoch 862: loss is 0.10912483185529709\n",
      "epoch 863: loss is 0.10912029445171356\n",
      "epoch 864: loss is 0.10911554098129272\n",
      "epoch 865: loss is 0.10911110043525696\n",
      "epoch 866: loss is 0.10910634696483612\n",
      "epoch 867: loss is 0.10910186171531677\n",
      "epoch 868: loss is 0.10909716784954071\n",
      "epoch 869: loss is 0.10909267514944077\n",
      "epoch 870: loss is 0.1090879812836647\n",
      "epoch 871: loss is 0.10908353328704834\n",
      "epoch 872: loss is 0.10907897353172302\n",
      "epoch 873: loss is 0.10907457023859024\n",
      "epoch 874: loss is 0.10906997323036194\n",
      "epoch 875: loss is 0.1090654730796814\n",
      "epoch 876: loss is 0.1090608760714531\n",
      "epoch 877: loss is 0.10905637592077255\n",
      "epoch 878: loss is 0.10905196517705917\n",
      "epoch 879: loss is 0.10904750972986221\n",
      "epoch 880: loss is 0.10904297232627869\n",
      "epoch 881: loss is 0.10903847217559814\n",
      "epoch 882: loss is 0.1090339720249176\n",
      "epoch 883: loss is 0.10902939736843109\n",
      "epoch 884: loss is 0.10902515053749084\n",
      "epoch 885: loss is 0.10902062058448792\n",
      "epoch 886: loss is 0.10901618748903275\n",
      "epoch 887: loss is 0.10901165008544922\n",
      "epoch 888: loss is 0.10900727659463882\n",
      "epoch 889: loss is 0.10900270938873291\n",
      "epoch 890: loss is 0.10899853706359863\n",
      "epoch 891: loss is 0.1089940145611763\n",
      "epoch 892: loss is 0.10898963361978531\n",
      "epoch 893: loss is 0.1089850664138794\n",
      "epoch 894: loss is 0.10898074507713318\n",
      "epoch 895: loss is 0.10897617787122726\n",
      "epoch 896: loss is 0.10897190123796463\n",
      "epoch 897: loss is 0.10896727442741394\n",
      "epoch 898: loss is 0.10896290838718414\n",
      "epoch 899: loss is 0.10895825922489166\n",
      "epoch 900: loss is 0.10895387083292007\n",
      "epoch 901: loss is 0.10894942283630371\n",
      "epoch 902: loss is 0.1089450940489769\n",
      "epoch 903: loss is 0.10894052684307098\n",
      "epoch 904: loss is 0.1089361384510994\n",
      "epoch 905: loss is 0.10893156379461288\n",
      "epoch 906: loss is 0.1089271679520607\n",
      "epoch 907: loss is 0.10892283916473389\n",
      "epoch 908: loss is 0.10891847312450409\n",
      "epoch 909: loss is 0.10891392827033997\n",
      "epoch 910: loss is 0.10890954732894897\n",
      "epoch 911: loss is 0.10890503972768784\n",
      "epoch 912: loss is 0.1089007630944252\n",
      "epoch 913: loss is 0.10889636725187302\n",
      "epoch 914: loss is 0.10889200866222382\n",
      "epoch 915: loss is 0.10888750851154327\n",
      "epoch 916: loss is 0.10888313502073288\n",
      "epoch 917: loss is 0.10887868702411652\n",
      "epoch 918: loss is 0.10887450724840164\n",
      "epoch 919: loss is 0.10887005180120468\n",
      "epoch 920: loss is 0.10886568576097488\n",
      "epoch 921: loss is 0.1088612973690033\n",
      "epoch 922: loss is 0.10885688662528992\n",
      "epoch 923: loss is 0.10885263979434967\n",
      "epoch 924: loss is 0.10884834080934525\n",
      "epoch 925: loss is 0.10884395241737366\n",
      "epoch 926: loss is 0.10883956402540207\n",
      "epoch 927: loss is 0.10883522778749466\n",
      "epoch 928: loss is 0.10883088409900665\n",
      "epoch 929: loss is 0.10882672667503357\n",
      "epoch 930: loss is 0.10882233083248138\n",
      "epoch 931: loss is 0.10881801694631577\n",
      "epoch 932: loss is 0.10881362110376358\n",
      "epoch 933: loss is 0.10880932211875916\n",
      "epoch 934: loss is 0.10880513489246368\n",
      "epoch 935: loss is 0.10880087316036224\n",
      "epoch 936: loss is 0.10879650712013245\n",
      "epoch 937: loss is 0.10879223048686981\n",
      "epoch 938: loss is 0.10878785699605942\n",
      "epoch 939: loss is 0.10878368467092514\n",
      "epoch 940: loss is 0.10877948254346848\n",
      "epoch 941: loss is 0.10877517610788345\n",
      "epoch 942: loss is 0.10877083241939545\n",
      "epoch 943: loss is 0.1087665855884552\n",
      "epoch 944: loss is 0.10876230895519257\n",
      "epoch 945: loss is 0.10875823348760605\n",
      "epoch 946: loss is 0.10875390470027924\n",
      "epoch 947: loss is 0.10874967277050018\n",
      "epoch 948: loss is 0.10874532163143158\n",
      "epoch 949: loss is 0.10874112695455551\n",
      "epoch 950: loss is 0.108737051486969\n",
      "epoch 951: loss is 0.10873282700777054\n",
      "epoch 952: loss is 0.10872848331928253\n",
      "epoch 953: loss is 0.10872434824705124\n",
      "epoch 954: loss is 0.10871999710798264\n",
      "epoch 955: loss is 0.10871601104736328\n",
      "epoch 956: loss is 0.10871174931526184\n",
      "epoch 957: loss is 0.10870759189128876\n",
      "epoch 958: loss is 0.10870327055454254\n",
      "epoch 959: loss is 0.10869914293289185\n",
      "epoch 960: loss is 0.10869505256414413\n",
      "epoch 961: loss is 0.10869093239307404\n",
      "epoch 962: loss is 0.1086866706609726\n",
      "epoch 963: loss is 0.1086825430393219\n",
      "epoch 964: loss is 0.10867825895547867\n",
      "epoch 965: loss is 0.10867427289485931\n",
      "epoch 966: loss is 0.108670175075531\n",
      "epoch 967: loss is 0.10866602510213852\n",
      "epoch 968: loss is 0.10866181552410126\n",
      "epoch 969: loss is 0.10865772515535355\n",
      "epoch 970: loss is 0.10865364223718643\n",
      "epoch 971: loss is 0.10864965617656708\n",
      "epoch 972: loss is 0.10864545404911041\n",
      "epoch 973: loss is 0.10864138603210449\n",
      "epoch 974: loss is 0.10863716155290604\n",
      "epoch 975: loss is 0.10863326489925385\n",
      "epoch 976: loss is 0.10862918943166733\n",
      "epoch 977: loss is 0.1086251437664032\n",
      "epoch 978: loss is 0.10862096399068832\n",
      "epoch 979: loss is 0.10861694067716599\n",
      "epoch 980: loss is 0.10861289501190186\n",
      "epoch 981: loss is 0.10860896855592728\n",
      "epoch 982: loss is 0.10860486328601837\n",
      "epoch 983: loss is 0.10860081017017365\n",
      "epoch 984: loss is 0.10859668999910355\n",
      "epoch 985: loss is 0.10859279334545135\n",
      "epoch 986: loss is 0.1085887923836708\n",
      "epoch 987: loss is 0.10858473926782608\n",
      "epoch 988: loss is 0.10858064889907837\n",
      "epoch 989: loss is 0.10857664048671722\n",
      "epoch 990: loss is 0.10857271403074265\n",
      "epoch 991: loss is 0.10856877267360687\n",
      "epoch 992: loss is 0.10856468230485916\n",
      "epoch 993: loss is 0.10856065899133682\n",
      "epoch 994: loss is 0.1085565835237503\n",
      "epoch 995: loss is 0.10855281352996826\n",
      "epoch 996: loss is 0.10854881256818771\n",
      "epoch 997: loss is 0.10854478180408478\n",
      "epoch 998: loss is 0.10854075103998184\n",
      "epoch 999: loss is 0.1085367277264595\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, my MLP model is able to outperform the linear model as it has a lower loss value. This means that my MLP model has a higher accuracy than the linear model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
