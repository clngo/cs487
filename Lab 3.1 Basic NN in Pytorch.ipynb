{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3.1: Basic Neural Network in PyTorch - Solution\n",
    "\n",
    "Let's create a linear classifier one more time, but using PyTorch's automatic differentiation and optimization algorithms.  Then you will extend the perceptron into a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitly tell PyTorch when creating a tensor that we are interested in later computing its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(5.,requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor(6.,requires_grad=True)\n",
    "c = 2*a+3*b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the gradients, we first need to call `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the gradient of any variable with respect to `c`, we simply access the `grad` attribute of that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and format the Palmer penguins dataset for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_penguins()\n",
    "\n",
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# get two features\n",
    "X = df[['flipper_length_mm','bill_length_mm']].values\n",
    "\n",
    "# convert species labels to integers\n",
    "y = df['species'].map({'Adelie':0,'Chinstrap':1,'Gentoo':2}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the learning algorithm work more smoothly, we we will subtract the mean of each feature.\n",
    "\n",
    "Here `np.mean` calculates a mean, and `axis=0` tells NumPy to calculate the mean over the rows (calculate the mean of each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.mean(X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our `X` and `y` arrays to torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Sequential` class creates a feed-forward network from a list of `nn.Module` objects.  Here we provide a single `nn.Linear` class which performs an affine transformation ($Wx+b$) so that we will have a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,3), # two inputs, three outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a cross-entropy loss function object and a stochastic gradient descent (SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(linear_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can iteratively optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 11.014744758605957\n",
      "epoch 1: loss is 8.893004417419434\n",
      "epoch 2: loss is 6.873161792755127\n",
      "epoch 3: loss is 5.09170389175415\n",
      "epoch 4: loss is 3.690431594848633\n",
      "epoch 5: loss is 2.5843398571014404\n",
      "epoch 6: loss is 1.7286109924316406\n",
      "epoch 7: loss is 1.189647912979126\n",
      "epoch 8: loss is 0.9589129686355591\n",
      "epoch 9: loss is 0.8545036315917969\n",
      "epoch 10: loss is 0.7861957550048828\n",
      "epoch 11: loss is 0.7327059507369995\n",
      "epoch 12: loss is 0.6872875690460205\n",
      "epoch 13: loss is 0.6471538543701172\n",
      "epoch 14: loss is 0.6109429597854614\n",
      "epoch 15: loss is 0.5779216885566711\n",
      "epoch 16: loss is 0.5476758480072021\n",
      "epoch 17: loss is 0.5199657082557678\n",
      "epoch 18: loss is 0.4946480989456177\n",
      "epoch 19: loss is 0.4716269373893738\n",
      "epoch 20: loss is 0.4508169889450073\n",
      "epoch 21: loss is 0.4321153461933136\n",
      "epoch 22: loss is 0.4153860807418823\n",
      "epoch 23: loss is 0.4004614055156708\n",
      "epoch 24: loss is 0.3871553838253021\n",
      "epoch 25: loss is 0.3752804696559906\n",
      "epoch 26: loss is 0.3646599054336548\n",
      "epoch 27: loss is 0.3551332950592041\n",
      "epoch 28: loss is 0.346558153629303\n",
      "epoch 29: loss is 0.3388095796108246\n",
      "epoch 30: loss is 0.331778347492218\n",
      "epoch 31: loss is 0.32536938786506653\n",
      "epoch 32: loss is 0.31950074434280396\n",
      "epoch 33: loss is 0.31410133838653564\n",
      "epoch 34: loss is 0.3091105818748474\n",
      "epoch 35: loss is 0.30447638034820557\n",
      "epoch 36: loss is 0.3001546263694763\n",
      "epoch 37: loss is 0.2961076498031616\n",
      "epoch 38: loss is 0.2923036217689514\n",
      "epoch 39: loss is 0.2887154221534729\n",
      "epoch 40: loss is 0.28532013297080994\n",
      "epoch 41: loss is 0.28209808468818665\n",
      "epoch 42: loss is 0.2790326774120331\n",
      "epoch 43: loss is 0.2761094570159912\n",
      "epoch 44: loss is 0.27331599593162537\n",
      "epoch 45: loss is 0.27064162492752075\n",
      "epoch 46: loss is 0.2680768668651581\n",
      "epoch 47: loss is 0.26561346650123596\n",
      "epoch 48: loss is 0.2632441818714142\n",
      "epoch 49: loss is 0.2609625458717346\n",
      "epoch 50: loss is 0.25876274704933167\n",
      "epoch 51: loss is 0.2566395699977875\n",
      "epoch 52: loss is 0.25458836555480957\n",
      "epoch 53: loss is 0.2526048421859741\n",
      "epoch 54: loss is 0.2506851851940155\n",
      "epoch 55: loss is 0.248825803399086\n",
      "epoch 56: loss is 0.24702347815036774\n",
      "epoch 57: loss is 0.24527522921562195\n",
      "epoch 58: loss is 0.2435782551765442\n",
      "epoch 59: loss is 0.2419300228357315\n",
      "epoch 60: loss is 0.24032814800739288\n",
      "epoch 61: loss is 0.23877055943012238\n",
      "epoch 62: loss is 0.2372550368309021\n",
      "epoch 63: loss is 0.2357797920703888\n",
      "epoch 64: loss is 0.23434294760227203\n",
      "epoch 65: loss is 0.23294293880462646\n",
      "epoch 66: loss is 0.23157811164855957\n",
      "epoch 67: loss is 0.23024694621562958\n",
      "epoch 68: loss is 0.2289482057094574\n",
      "epoch 69: loss is 0.2276804894208908\n",
      "epoch 70: loss is 0.2264426052570343\n",
      "epoch 71: loss is 0.22523337602615356\n",
      "epoch 72: loss is 0.22405171394348145\n",
      "epoch 73: loss is 0.22289659082889557\n",
      "epoch 74: loss is 0.22176702320575714\n",
      "epoch 75: loss is 0.22066207230091095\n",
      "epoch 76: loss is 0.21958088874816895\n",
      "epoch 77: loss is 0.21852262318134308\n",
      "epoch 78: loss is 0.21748650074005127\n",
      "epoch 79: loss is 0.21647171676158905\n",
      "epoch 80: loss is 0.21547763049602509\n",
      "epoch 81: loss is 0.21450352668762207\n",
      "epoch 82: loss is 0.2135487198829651\n",
      "epoch 83: loss is 0.21261267364025116\n",
      "epoch 84: loss is 0.21169471740722656\n",
      "epoch 85: loss is 0.2107943296432495\n",
      "epoch 86: loss is 0.20991097390651703\n",
      "epoch 87: loss is 0.2090441733598709\n",
      "epoch 88: loss is 0.20819337666034698\n",
      "epoch 89: loss is 0.20735815167427063\n",
      "epoch 90: loss is 0.20653802156448364\n",
      "epoch 91: loss is 0.2057325839996338\n",
      "epoch 92: loss is 0.20494139194488525\n",
      "epoch 93: loss is 0.20416408777236938\n",
      "epoch 94: loss is 0.20340028405189514\n",
      "epoch 95: loss is 0.20264962315559387\n",
      "epoch 96: loss is 0.20191174745559692\n",
      "epoch 97: loss is 0.20118628442287445\n",
      "epoch 98: loss is 0.20047296583652496\n",
      "epoch 99: loss is 0.1997714787721634\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = linear_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Extend the above code to implement an MLP with a single hidden layer of size 100.\n",
    "\n",
    "Write code to compute the accuracy of each model.\n",
    "\n",
    "Can you get the MLP to outperform the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new changes\n",
    "mlp_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 100), # 2 inputs, 1 hidden layer of size 100\n",
    "    \n",
    "    # hidden activation function, the magic happens\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    torch.nn.Linear(100, 3) # 100 inputs, 3 outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yippie, we have the model\n",
    "\n",
    "# time to optimize the loss function\n",
    "lr = 1e-2\n",
    "opt = torch.optim.SGD(mlp_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 1.6419166326522827\n",
      "epoch 1: loss is 0.6736262440681458\n",
      "epoch 2: loss is 0.504383385181427\n",
      "epoch 3: loss is 0.39080289006233215\n",
      "epoch 4: loss is 0.320997416973114\n",
      "epoch 5: loss is 0.2786753177642822\n",
      "epoch 6: loss is 0.2524169087409973\n",
      "epoch 7: loss is 0.23527199029922485\n",
      "epoch 8: loss is 0.22316941618919373\n",
      "epoch 9: loss is 0.21385875344276428\n",
      "epoch 10: loss is 0.20623008906841278\n",
      "epoch 11: loss is 0.1997596174478531\n",
      "epoch 12: loss is 0.19416135549545288\n",
      "epoch 13: loss is 0.18925464153289795\n",
      "epoch 14: loss is 0.18491105735301971\n",
      "epoch 15: loss is 0.18103379011154175\n",
      "epoch 16: loss is 0.17754794657230377\n",
      "epoch 17: loss is 0.17439445853233337\n",
      "epoch 18: loss is 0.17152613401412964\n",
      "epoch 19: loss is 0.16890431940555573\n",
      "epoch 20: loss is 0.16649754345417023\n",
      "epoch 21: loss is 0.16427981853485107\n",
      "epoch 22: loss is 0.1622294932603836\n",
      "epoch 23: loss is 0.16032828390598297\n",
      "epoch 24: loss is 0.15856070816516876\n",
      "epoch 25: loss is 0.15691353380680084\n",
      "epoch 26: loss is 0.1553753912448883\n",
      "epoch 27: loss is 0.15393641591072083\n",
      "epoch 28: loss is 0.15258793532848358\n",
      "epoch 29: loss is 0.1513223797082901\n",
      "epoch 30: loss is 0.1501331627368927\n",
      "epoch 31: loss is 0.14901429414749146\n",
      "epoch 32: loss is 0.14796055853366852\n",
      "epoch 33: loss is 0.14696715772151947\n",
      "epoch 34: loss is 0.14602985978126526\n",
      "epoch 35: loss is 0.14514479041099548\n",
      "epoch 36: loss is 0.14430829882621765\n",
      "epoch 37: loss is 0.1435171514749527\n",
      "epoch 38: loss is 0.14276835322380066\n",
      "epoch 39: loss is 0.14205916225910187\n",
      "epoch 40: loss is 0.14138706028461456\n",
      "epoch 41: loss is 0.14074967801570892\n",
      "epoch 42: loss is 0.14014476537704468\n",
      "epoch 43: loss is 0.13957034051418304\n",
      "epoch 44: loss is 0.13902445137500763\n",
      "epoch 45: loss is 0.13850535452365875\n",
      "epoch 46: loss is 0.1380113810300827\n",
      "epoch 47: loss is 0.13754095137119293\n",
      "epoch 48: loss is 0.13709256052970886\n",
      "epoch 49: loss is 0.13666492700576782\n",
      "epoch 50: loss is 0.13625675439834595\n",
      "epoch 51: loss is 0.13586682081222534\n",
      "epoch 52: loss is 0.13549400866031647\n",
      "epoch 53: loss is 0.13513731956481934\n",
      "epoch 54: loss is 0.13479577004909515\n",
      "epoch 55: loss is 0.1344684362411499\n",
      "epoch 56: loss is 0.13415442407131195\n",
      "epoch 57: loss is 0.13385294377803802\n",
      "epoch 58: loss is 0.13356326520442963\n",
      "epoch 59: loss is 0.13328465819358826\n",
      "epoch 60: loss is 0.13301648199558258\n",
      "epoch 61: loss is 0.13275815546512604\n",
      "epoch 62: loss is 0.13250908255577087\n",
      "epoch 63: loss is 0.1322687864303589\n",
      "epoch 64: loss is 0.13203665614128113\n",
      "epoch 65: loss is 0.13181239366531372\n",
      "epoch 66: loss is 0.13159549236297607\n",
      "epoch 67: loss is 0.13138556480407715\n",
      "epoch 68: loss is 0.13118219375610352\n",
      "epoch 69: loss is 0.1309850960969925\n",
      "epoch 70: loss is 0.13079394400119781\n",
      "epoch 71: loss is 0.1306084245443344\n",
      "epoch 72: loss is 0.1304282397031784\n",
      "epoch 73: loss is 0.13025318086147308\n",
      "epoch 74: loss is 0.13008299469947815\n",
      "epoch 75: loss is 0.1299174278974533\n",
      "epoch 76: loss is 0.12975625693798065\n",
      "epoch 77: loss is 0.12959933280944824\n",
      "epoch 78: loss is 0.12944646179676056\n",
      "epoch 79: loss is 0.12929745018482208\n",
      "epoch 80: loss is 0.1291520893573761\n",
      "epoch 81: loss is 0.12901026010513306\n",
      "epoch 82: loss is 0.1288718581199646\n",
      "epoch 83: loss is 0.1287367343902588\n",
      "epoch 84: loss is 0.1286046952009201\n",
      "epoch 85: loss is 0.12847565114498138\n",
      "epoch 86: loss is 0.12834961712360382\n",
      "epoch 87: loss is 0.12822632491588593\n",
      "epoch 88: loss is 0.12810568511486053\n",
      "epoch 89: loss is 0.1279875934123993\n",
      "epoch 90: loss is 0.12787193059921265\n",
      "epoch 91: loss is 0.12775865197181702\n",
      "epoch 92: loss is 0.12764760851860046\n",
      "epoch 93: loss is 0.1275387853384018\n",
      "epoch 94: loss is 0.12743209302425385\n",
      "epoch 95: loss is 0.12732739746570587\n",
      "epoch 96: loss is 0.12722469866275787\n",
      "epoch 97: loss is 0.12712392210960388\n",
      "epoch 98: loss is 0.1270250827074051\n",
      "epoch 99: loss is 0.12692809104919434\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    opt.zero_grad() # zero out the gradients\n",
    "\n",
    "    z = mlp_model(X) # compute z values\n",
    "    loss = loss_fn(z,y) # compute loss\n",
    "\n",
    "    loss.backward() # compute gradients\n",
    "\n",
    "    opt.step() # apply gradients\n",
    "\n",
    "    print(f'epoch {epoch}: loss is {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Classifier Accuracy 0.9309309309309309\n",
      "MLP Classifier Accuracy 0.9579579579579579\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy of the model\n",
    "def accuracy(model, X, y):\n",
    "    \n",
    "    # Set model to evaluation mode \n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    z = model(X)\n",
    "    \n",
    "    # First dimension of outputs are the samples (dim = 0)\n",
    "    # Second dimension of outputs are the labels (dim = 1)\n",
    "    # Get the highest predicted labels value for each sample in the \n",
    "    sample, predicted_labels = torch.max(z, dim=1)\n",
    "\n",
    "\n",
    "    # Calculate the accuracy (the number of correct predictions divided by total number of samples)\n",
    "    correct = (predicted_labels == y).sum().item()\n",
    "\n",
    "    # size(0) refers to first dimension, which are the samples (dim = 0)\n",
    "    total = y.size(0) \n",
    "\n",
    "    return correct/total\n",
    "\n",
    "print(f\"Linear Classifier Accuracy {accuracy(linear_model, X, y)}\")\n",
    "print(f\"MLP Classifier Accuracy {accuracy(mlp_model, X, y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
